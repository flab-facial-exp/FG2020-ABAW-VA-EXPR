{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluate multi time-window model taht is fusion model of multi target models\n",
    " * select type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * select sub1, sub2 type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * evaluate validation per frame\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sklearn #機械学習のライブラリ\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,f1_score\n",
    "from statistics import mean, median,variance,stdev\n",
    "import math\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base dataset: concat au csv & add label, file count & drop unnecessary columns\n",
    "#   str_substract_time: ex. '02s' , str_search_key: ex. '(Subject_).*.csv'\n",
    "#   cut_start: trim data (X sec from start), cut_end: trim data (X sec from end)\n",
    "def crate_base_data(data_file_names, str_type, str_time):\n",
    "    # create empty dataframe (base dataframe)\n",
    "    #data = pd.DataFrame()\n",
    "    count = 0\n",
    "    max_count = len(data_file_names)\n",
    "    \n",
    "    data_list = [pd.DataFrame()] # <- dummy\n",
    "    \n",
    "    for  data_file in data_file_names:\n",
    "        # read au csv\n",
    "        if os.path.isfile(data_file) and os.path.getsize(data_file) > 32:\n",
    "            #print(os.path.getsize(data_file))\n",
    "            #data_tmp = pd.read_csv(data_file)\n",
    "            data_tmp = pd.read_hdf(data_file)\n",
    "        else:\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        if (len(data_tmp)<1):\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        # create column - 'count', 'Label', 'subject' (default: 0)\n",
    "        data_tmp[\"count\"] = 0\n",
    "        data_tmp[\"subject\"] = \"sample\"\n",
    "\n",
    "        # convert filename to 'subject'\n",
    "        name_train = os.path.splitext(os.path.basename(data_file))[0].replace(str_time,'')\n",
    "        #print(name_train)\n",
    "\n",
    "        #print(data_temp)\n",
    "        # get and set Label value\n",
    "        data_tmp[\"count\"]  = count\n",
    "        data_tmp[\"subject\"] = name_train\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        # ' frame-avg',' face_id-avg,' timestamp-avg',' confidence-avg,' success-avg','frame-std',' face_id-std',' confidence-std',' success-std'\n",
    "        data_tmp = data_tmp.drop(['frame-avg',' face_id-avg',' timestamp-avg',' confidence-avg',' success-avg',\n",
    "                                  'frame-std',' face_id-std',' timestamp-std',' confidence-std',' success-std',\n",
    "                                  'frame-range', ' face_id-range', ' timestamp-range', ' confidence-range', ' success-range',\n",
    "                                  'frame-slope', ' face_id-slope', ' timestamp-slope', ' confidence-slope', ' success-slope',\n",
    "                                  'Unnamed: 0-avg', 'Unnamed: 0-std', 'Unnamed: 0-range', 'Unnamed: 0-slope'\n",
    "                               ], axis=1)\n",
    "        if str_type == \"EXP\":\n",
    "            data_tmp = data_tmp.drop(['Neutral-std','Neutral-range','Neutral-slope'], axis=1)\n",
    "        else:\n",
    "            data_tmp = data_tmp.drop(['arousal-std', 'arousal-range', 'arousal-slope', \n",
    "                                     'valence-std', 'valence-range', 'valence-slope'], axis=1)\n",
    "\n",
    "        # append created data to base dataframe\n",
    "        #data = data.append(data_tmp)\n",
    "        data_list.append(data_tmp)\n",
    "\n",
    "        log = 'count: {0}, name: {1}, data shape: {2}'.format(count, name_train, data_tmp.shape)\n",
    "        print(log)\n",
    "        count = count + 1\n",
    "    # finish\n",
    "    del data_list[0]\n",
    "    data = pd.concat([x for x in data_list])\n",
    "    \n",
    "    log = '**** finished creating base dataset, data shape: {0}'.format(data.shape)\n",
    "    print(log)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_model):\n",
    "    with open(file_model, mode='rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split base data to <au>, <gaze and pose>, <eye_landmark, 2d landmark, 3d landmark>\n",
    "# ** 'count','label','subject' is contained in all splits\n",
    "def split_data(in_data):\n",
    "    # au data\n",
    "    df_au = in_data.loc[:, in_data.columns.str.contains(\"AU|count|subject|Neutral|valence|arousal\") ]\n",
    "    #df_au = df_au.join(df_lable)\n",
    "    print(\"AU data shape: \",df_au.shape)\n",
    "\n",
    "    # gaze and pose data **** temp pose\n",
    "    df_pose = in_data.loc[:, in_data.columns.str.contains(\"pose_|count|subject|Neutral|valence|arousal\") ]\n",
    "    #df_pose = df_pose.join(df_lable)\n",
    "    print(\"Gaze & Pose data shape: \",df_pose.shape)\n",
    "    \n",
    "    # eye_landmark, 2d landmark, 3d landmark data **** temp gaze\n",
    "    df_lmk = in_data.loc[:, in_data.columns.str.contains(\"gaze|count|subject|Neutral|valence|arousal\")]\n",
    "    #df_lmk = df_lmk.join(df_lable)\n",
    "    print(\"Landmark data shape: \",df_lmk.shape)\n",
    "    \n",
    "    # openpose\n",
    "    #df_op = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze\")]\n",
    "    df_op = in_data.loc[:, in_data.columns.str.contains(\"hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c|count|subject|Neutral|valence|arousal\")]\n",
    "    print(\"Opepose data shape: \",df_op.shape)\n",
    "    \n",
    "    # resnet\n",
    "    df_rn = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze|hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c\")]\n",
    "    print(\"Resnet data shape: \",df_rn.shape)\n",
    "    \n",
    "    print(\"** end **\")\n",
    "    return df_au,df_pose,df_lmk,df_op, df_rn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for single time analysis (Light GBM ...)\n",
    "def make_dataset_for_gbm(in_data, str_type):\n",
    "    \n",
    "    if str_type == \"EXP\":\n",
    "        in_data = in_data[in_data[\"Neutral-avg\"]>=0]\n",
    "        # EXP\n",
    "        # spplit features , labels\n",
    "        data_y = in_data.loc[:,[\"count\", \"subject\", \"Neutral-avg\"]]\n",
    "        data_x = in_data.drop([\"count\", \"subject\", \"Neutral-avg\"], axis=1)\n",
    "    else:\n",
    "        # VA\n",
    "        # spplit features , labels\n",
    "        data_y = in_data.loc[:,[\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"]]\n",
    "        data_x = in_data.drop([\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"], axis=1)\n",
    "    \n",
    "    dim = len(data_x.columns)\n",
    "    \n",
    "    # drop 'count','group' from data_y\n",
    "    data_y = data_y.drop([\"count\", \"subject\"], axis=1) \n",
    "    if str_type == \"VA_A\":\n",
    "        data_y = data_y.drop([\"valence-avg\"], axis=1) \n",
    "    elif str_type == \"VA_V\":\n",
    "        data_y = data_y.drop([\"arousal-avg\"], axis=1) \n",
    "    \n",
    "    # convert pandas to numpy \n",
    "    np_data_x = data_x.values\n",
    "    np_data_y = data_y.values\n",
    "    \n",
    "    # reshape data for tda\n",
    "    np_data_x = np.reshape(np_data_x, [len(np_data_y),dim])\n",
    "    np_data_y = np.reshape(np_data_y, [len(np_data_y),1])\n",
    "\n",
    "    return np_data_x, np_data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for single time analysis (Light GBM ...)\n",
    "def make_dataset_for_gbm_sub(in_data, str_type):\n",
    "    \n",
    "    data_x = in_data.drop([\"count\", \"subject\"], axis=1)\n",
    "    if \"Neutral-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"Neutral-avg\"], axis=1)\n",
    "    if \"valence-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"valence-avg\"], axis=1)\n",
    "    if \"arousal-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"arousal-avg\"], axis=1)\n",
    "    \n",
    "    dim = len(data_x.columns)\n",
    "    length = len(data_x)\n",
    "    \n",
    "    # convert pandas to numpy \n",
    "    np_data_x = data_x.values\n",
    "    \n",
    "    # reshape data for tda\n",
    "    np_data_x = np.reshape(np_data_x, [length, dim])\n",
    "    \n",
    "    return np_data_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_data_val_time(data_val, models, str_type, window_time, dir_features = None):\n",
    "    log = \"split data to AU ,pose, gaze, openpose\"\n",
    "    print(log)\n",
    "    val_au, val_pose, val_lmk, val_op, val_rn = split_data(data_val)\n",
    "    \n",
    "    if dir_features != None:\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_au.csv\"\n",
    "        val_au = substruct_features(val_au, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_pose.csv\"\n",
    "        val_pose = substruct_features(val_pose, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_lmk.csv\"\n",
    "        val_lmk = substruct_features(val_lmk, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_op.csv\"\n",
    "        val_op = substruct_features(val_op, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_rn.csv\"\n",
    "        val_rn = substruct_features(val_rn, file_f, str_type, window_time)\n",
    "    \n",
    "    log = \"convert data(pandas) to data(numpy) for LightGBM\"\n",
    "    print(log)\n",
    "    \n",
    "    np_val_au_x, np_val_au_y = make_dataset_for_gbm(val_au, str_type)\n",
    "    np_val_pose_x, np_val_pose_y = make_dataset_for_gbm(val_pose, str_type)\n",
    "    np_val_lmk_x, np_val_lmk_y = make_dataset_for_gbm(val_lmk, str_type)\n",
    "    np_val_op_x, np_val_op_y = make_dataset_for_gbm(val_op, str_type)\n",
    "    np_val_rn_x, np_val_rn_y = make_dataset_for_gbm(val_rn, str_type)\n",
    "\n",
    "    log = \"predict by au, pose, gaze, openpose, GAPP ensemple\"\n",
    "    print(log)\n",
    "        \n",
    "    if str_type == \"EXP\":\n",
    "        pred_val_au = models[0].predict(np_val_au_x)\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x)\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x)\n",
    "        pred_val_op = models[3].predict(np_val_op_x)\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x)\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk,\n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x)\n",
    "        np_val_true = np_val_au_y.ravel()\n",
    "    else:\n",
    "        pred_val_au = models[0].predict(np_val_au_x).ravel()\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x).ravel()\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x).ravel()\n",
    "        pred_val_op = models[3].predict(np_val_op_x).ravel()\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x).ravel()\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk, \n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x).ravel()\n",
    "        np_val_true = np_val_au_y.ravel()\n",
    "    \n",
    "    return np_val_true, pred_val_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_data_val_time_sub(data_val, models, str_type, main_type, window_time, dir_features = None):\n",
    "    log = \"split data to AU ,pose, gaze, openpose\"\n",
    "    print(log)\n",
    "    val_au, val_pose, val_lmk, val_op, val_rn = split_data(data_val)\n",
    "    \n",
    "    if dir_features != None:\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_au.csv\"\n",
    "        val_au = substruct_features(val_au, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_pose.csv\"\n",
    "        val_pose = substruct_features(val_pose, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_lmk.csv\"\n",
    "        val_lmk = substruct_features(val_lmk, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_op.csv\"\n",
    "        val_op = substruct_features(val_op, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_rn.csv\"\n",
    "        val_rn = substruct_features(val_rn, file_f, main_type, window_time)\n",
    "    \n",
    "    log = \"convert data(pandas) to data(numpy) for LightGBM\"\n",
    "    print(log)\n",
    "    \n",
    "    np_val_au_x = make_dataset_for_gbm_sub(val_au, main_type)\n",
    "    np_val_pose_x = make_dataset_for_gbm_sub(val_pose, main_type)\n",
    "    np_val_lmk_x = make_dataset_for_gbm_sub(val_lmk, main_type)\n",
    "    np_val_op_x = make_dataset_for_gbm_sub(val_op, main_type)\n",
    "    np_val_rn_x = make_dataset_for_gbm_sub(val_rn, main_type)\n",
    "\n",
    "    log = \"predict by au, pose, gaze, openpose, GAPP ensemple\"\n",
    "    print(log)\n",
    "    print(str_type)\n",
    "        \n",
    "    if str_type == \"EXP\":\n",
    "        pred_val_au = models[0].predict(np_val_au_x)\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x)\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x)\n",
    "        pred_val_op = models[3].predict(np_val_op_x)\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x)\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk,\n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x)\n",
    "    else:\n",
    "        pred_val_au = models[0].predict(np_val_au_x).ravel()\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x).ravel()\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x).ravel()\n",
    "        pred_val_op = models[3].predict(np_val_op_x).ravel()\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x).ravel()\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk, \n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x).ravel()\n",
    "    \n",
    "    return pred_val_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predict data\n",
    "def eval_pred(data_true, data_pred, str_type):\n",
    "    score_f1 = 0\n",
    "    score_acc = 0\n",
    "\n",
    "    # if target is expression, calc F1 score, accuracy\n",
    "    if str_type == \"EXP\":\n",
    "        # convert to 7-columns predict probability to 1-column predict\n",
    "        pred_tmp = np.argmax(data_pred, axis=1) # 一番大きい予測確率のクラスを予測クラスに\n",
    "        \n",
    "        ltrue = list(data_true)\n",
    "        lpred = list(pred_tmp) \n",
    "        \n",
    "        score_1 = f1_score(ltrue, lpred, average='macro') # weighted, macro, micro\n",
    "        score_2 = accuracy_score(ltrue, lpred)\n",
    "    # if target is VA, calc CCC, mse\n",
    "    else:\n",
    "        pred_tmp = data_pred\n",
    "        #pred_tmp = data_pred.round()\n",
    "        x_mean = pred_tmp.mean()\n",
    "        y_mean = data_true.mean()\n",
    "        sx2 = ((pred_tmp-x_mean)*(pred_tmp-x_mean)).mean()\n",
    "        sy2 = ((data_true-y_mean)*(data_true-y_mean)).mean()\n",
    "        sxy = ((pred_tmp-x_mean)*(data_true-y_mean)).mean()\n",
    "        score_1 = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "        #score_acc = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "        score_2 = mean_squared_error(data_true, data_pred)\n",
    "    return score_1, score_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_models(dir_model, str_type, window_time):\n",
    "    ext = \"_{0}s.pickle\".format(str(window_time).zfill(2))\n",
    "    \n",
    "    model_au = load_model(dir_model + \"model_au_gbm_\" + str_type + ext)\n",
    "    model_pose = load_model(dir_model + \"model_pose_gbm_\" + str_type + ext)\n",
    "    model_lmk = load_model(dir_model + \"model_lmk_gbm_\" + str_type + ext)\n",
    "    model_op = load_model(dir_model + \"model_op_gbm_\" + str_type + ext)\n",
    "    model_rn = load_model(dir_model + \"model_rn_gbm_\" + str_type + ext)\n",
    "    model_ens = load_model(dir_model + \"model_ens_gbm_\" + str_type + ext)\n",
    "    \n",
    "    models = [model_au, model_pose, model_lmk, model_op, model_rn, model_ens]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substruct_features(data, fp, str_type, window_time):\n",
    "    col = str(window_time).zfill(2) + \"s\"\n",
    "    data_f = pd.read_csv(fp)\n",
    "    \n",
    "    if str_type == \"EXP\":\n",
    "        list_f = list(data_f[col].values.ravel())\n",
    "        list_f = np.append(list_f, [\"count\", \"subject\", \"Neutral-avg\"])\n",
    "    else:\n",
    "        list_f = list(data_f[col].values.ravel())\n",
    "        list_f = np.append(list_f, [\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"])\n",
    "    \n",
    "    data2 = data[list_f]\n",
    "    \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation_frame(dir_validation, dir_model_01s, dir_model_06s, dir_model_12s, dir_model_fusion,\n",
    "                         dir_out, target, target_sub1, target_sub2, dir_features=None):\n",
    "    str_type = target\n",
    "    str_type_sub1 = target_sub1\n",
    "    str_type_sub2 = target_sub2\n",
    "    \n",
    "    log = \"Target type: {0}\".format(str_type)\n",
    "    print(log)\n",
    "    \n",
    "    # read models\n",
    "    model_01s = read_models(dir_model_01s, str_type, 1)\n",
    "    model_06s = read_models(dir_model_06s, str_type, 6)\n",
    "    model_12s = read_models(dir_model_12s, str_type, 12)\n",
    "    \n",
    "    model_01s_sub1 = read_models(dir_model_01s, str_type_sub1, 1)\n",
    "    model_06s_sub1 = read_models(dir_model_06s, str_type_sub1, 6)\n",
    "    model_12s_sub1 = read_models(dir_model_12s, str_type_sub1, 12)\n",
    "    \n",
    "    model_01s_sub2 = read_models(dir_model_01s, str_type_sub2, 1)\n",
    "    model_06s_sub2 = read_models(dir_model_06s, str_type_sub2, 6)\n",
    "    model_12s_sub2 = read_models(dir_model_12s, str_type_sub2, 12)\n",
    "    \n",
    "    model_fusion = load_model(dir_model_fusion + \"model_fusion_multi_\" + str_type + \".pickle\")\n",
    "    model_fusion_sub1 = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type_sub1 + \".pickle\")\n",
    "    model_fusion_sub2 = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type_sub2 + \".pickle\")\n",
    "    \n",
    "    # search files of validation data\n",
    "    file_val = dir_validation + \"*_01s.h5\"\n",
    "    files_val_01s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 01s: {0}\".format(len(files_val_01s))\n",
    "    print(log)\n",
    "\n",
    "    file_val = dir_validation + \"*_06s.h5\"\n",
    "    files_val_06s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 06s: {0}\".format(len(files_val_06s))\n",
    "    print(log)\n",
    "\n",
    "    file_val = dir_validation + \"*_12s.h5\"\n",
    "    files_val_12s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 12s: {0}\".format(len(files_val_12s))\n",
    "    print(log)\n",
    "    \n",
    "    # create base dataset\n",
    "    log = \"data loading....\"\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_01s\"\n",
    "    #data_train = pd.read_hdf(file_train, 'key')\n",
    "    data_val_01s = crate_base_data(files_val_01s, str_type, str_time)\n",
    "    log = \"data validation 01s shape: {0}\".format(data_val_01s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_06s\"\n",
    "    data_val_06s = crate_base_data(files_val_06s, str_type, str_time)\n",
    "    log = \"data validation 06s shape: {0}\".format(data_val_06s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_12s\"\n",
    "    data_val_12s = crate_base_data(files_val_12s, str_type, str_time)\n",
    "    log = \"data validation 12s shape: {0}\".format(data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    #data_val = pd.read_hdf(file_val, 'key')\n",
    "\n",
    "    # create base dataset\n",
    "    log = \"finished data loading\"\n",
    "    print(log)\n",
    "    \n",
    "    # adjust data shape (same frame)\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}\".format(data_val_01s.shape, data_val_06s.shape, data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    length_columns = len(data_val_01s.columns)\n",
    "    base_columns = data_val_01s.columns\n",
    "\n",
    "    data_val_01s.columns = data_val_01s.columns + \"_01s\"\n",
    "    data_val_06s.columns = data_val_06s.columns + \"_06s\"\n",
    "    data_val_12s.columns = data_val_12s.columns + \"_12s\"\n",
    "\n",
    "    data_val = pd.concat([data_val_01s, data_val_06s, data_val_12s], axis=1)\n",
    "    if str_type == \"EXP\":\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_01s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_06s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_12s\"]>=0]\n",
    "    data_val = data_val.dropna(how='any')\n",
    "    val_index = data_val.index\n",
    "\n",
    "    data_val_01s = data_val.iloc[:,0:length_columns]\n",
    "    data_val_06s = data_val.iloc[:,length_columns:length_columns*2]\n",
    "    data_val_12s = data_val.iloc[:,length_columns*2:length_columns*3]\n",
    "\n",
    "    data_val_01s.columns = base_columns\n",
    "    data_val_06s.columns = base_columns\n",
    "    data_val_12s.columns = base_columns\n",
    "\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}\".format(data_val_01s.shape, data_val_06s.shape, data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # main\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    np_val_true_01s, pred_val_ens_01s = predict_data_val_time(data_val_01s, model_01s, str_type, window_time, dir_features)\n",
    "    log = \"01s pred shape) val: {0}\".format(pred_val_ens_01s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    np_val_true_06s, pred_val_ens_06s = predict_data_val_time(data_val_06s, model_06s, str_type, window_time, dir_features)\n",
    "    log = \"06s pred shape) val: {0}\".format(pred_val_ens_06s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    np_val_true_12s, pred_val_ens_12s = predict_data_val_time(data_val_12s, model_12s, str_type, window_time, dir_features)\n",
    "    log = \"12s pred shape) val: {0}\".format(pred_val_ens_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "\n",
    "    score_1, score_2 = eval_pred(np_val_true_01s, pred_val_ens_01s, str_type)\n",
    "    if str_type == \"EXP\":\n",
    "        score = 0.67*score_1 + 0.33*score_2\n",
    "    else:\n",
    "        score = score_1\n",
    "    log1 = \"01s, score: {0}. score1: {1}, score2: {2}\".format(score, score_1, score_2)\n",
    "    score_1, score_2 = eval_pred(np_val_true_01s, pred_val_ens_06s, str_type)\n",
    "    if str_type == \"EXP\":\n",
    "        score = 0.67*score_1 + 0.33*score_2\n",
    "    else:\n",
    "        score = score_1\n",
    "    log2 = \"06s, score: {0}. score1: {1}, score2: {2}\".format(score, score_1, score_2)\n",
    "    score_1, score_2 = eval_pred(np_val_true_01s, pred_val_ens_12s, str_type)\n",
    "    if str_type == \"EXP\":\n",
    "        score = 0.67*score_1 + 0.33*score_2\n",
    "    else:\n",
    "        score = score_1\n",
    "    log3 = \"12s, score: {0}. score1: {1}, score2: {2}\".format(score, score_1, score_2)\n",
    "\n",
    "    # sub1 (with fusion single)\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    #pred_train_01s, pred_val_01s = predict_data(data_train_01s, data_val_01s, model_01s, str_type)\n",
    "    pred_val_ens_01s_sub1 = predict_data_val_time_sub(data_val_01s, model_01s_sub1, str_type_sub1, str_type, window_time, dir_features)\n",
    "    log = \"01s pred shape) val: {0}\".format(pred_val_ens_01s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    #pred_train_06s, pred_val_06s = predict_data(data_train_06s, data_val_06s, model_06s, str_type)\n",
    "    pred_val_ens_06s_sub1 = predict_data_val_time_sub(data_val_06s, model_06s_sub1, str_type_sub1, str_type, window_time, dir_features)\n",
    "    log = \"06s pred shape) val: {0}\".format(pred_val_ens_06s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    #pred_train_12s, pred_val_12s = predict_data(data_train_12s, data_val_12s, model_12s, str_type)\n",
    "    pred_val_ens_12s_sub1 = predict_data_val_time_sub(data_val_12s, model_12s_sub1, str_type_sub1, str_type, window_time, dir_features)\n",
    "    log = \"12s pred shape) val: {0}\".format(pred_val_ens_12s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "    # fusion sub1\n",
    "\n",
    "    np_val_x1 = pred_val_ens_01s_sub1\n",
    "    np_val_x2 = pred_val_ens_06s_sub1\n",
    "    np_val_x3 = pred_val_ens_12s_sub1\n",
    "    #np_val_y  = np_val_true_01s_sub1\n",
    "\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((np_val_x1, np_val_x2, np_val_x3))\n",
    "\n",
    "    if str_type_sub1 == \"EXP\":\n",
    "        #pred_train_sub1 = model_fusion.predict(stack_pred)\n",
    "        pred_val_sub1 = model_fusion_sub1.predict(stack_pred_val)\n",
    "    else:\n",
    "        #pred_train_sub1 = model_fusion.predict(stack_pred).ravel()\n",
    "        pred_val_sub1 = model_fusion_sub1.predict(stack_pred_val).ravel()\n",
    "\n",
    "    # sub2 (with fusion single)\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    #pred_train_01s, pred_val_01s = predict_data(data_train_01s, data_val_01s, model_01s, str_type)\n",
    "    pred_val_ens_01s_sub2 = predict_data_val_time_sub(data_val_01s, model_01s_sub2, str_type_sub2, str_type, window_time, dir_features)\n",
    "    log = \"01s pred shape) val: {0}\".format(pred_val_ens_01s_sub2.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    #pred_train_06s, pred_val_06s = predict_data(data_train_06s, data_val_06s, model_06s, str_type)\n",
    "    pred_val_ens_06s_sub2 = predict_data_val_time_sub(data_val_06s, model_06s_sub2, str_type_sub2, str_type, window_time, dir_features)\n",
    "    log = \"06s pred shape) val: {0}\".format(pred_val_ens_06s_sub2.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    #pred_train_12s, pred_val_12s = predict_data(data_train_12s, data_val_12s, model_12s, str_type)\n",
    "    pred_val_ens_12s_sub2 = predict_data_val_time_sub(data_val_12s, model_12s_sub2, str_type_sub2, str_type, window_time, dir_features)\n",
    "    log = \"12s pred shape) val: {0}\".format(pred_val_ens_12s_sub2.shape)\n",
    "    print(log)\n",
    "\n",
    "    # fusion sub1\n",
    "\n",
    "    np_val_x1 = pred_val_ens_01s_sub2\n",
    "    np_val_x2 = pred_val_ens_06s_sub2\n",
    "    np_val_x3 = pred_val_ens_12s_sub2\n",
    "    #np_val_y  = np_val_true_01s_sub2\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((np_val_x1, np_val_x2, np_val_x3))\n",
    "\n",
    "    if str_type_sub2 == \"EXP\":\n",
    "        #pred_train_sub2 = model_fusion.predict(stack_pred)\n",
    "        pred_val_sub2 = model_fusion_sub2.predict(stack_pred_val)\n",
    "    else:\n",
    "        #pred_train_sub2 = model_fusion.predict(stack_pred).ravel()\n",
    "        pred_val_sub2 = model_fusion_sub2.predict(stack_pred_val).ravel()\n",
    "\n",
    "    # single ensemble ************** if needed\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((pred_val_ens_01s, pred_val_ens_06s, pred_val_ens_12s))\n",
    "\n",
    "    model_fusion_tmp = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type + \".pickle\")\n",
    "\n",
    "    if str_type == \"EXP\":\n",
    "        #pred_train_sub1 = model_fusion.predict(stack_pred)\n",
    "        pred_val = model_fusion_tmp.predict(stack_pred_val)\n",
    "    else:\n",
    "        #pred_train_sub1 = model_fusion.predict(stack_pred).ravel()\n",
    "        pred_val = model_fusion_tmp.predict(stack_pred_val).ravel()\n",
    "\n",
    "    score_1, score_2 = eval_pred(np_val_true_01s, pred_val, str_type)\n",
    "    if str_type == \"EXP\":\n",
    "        score = 0.67*score_1 + 0.33*score_2\n",
    "    else:\n",
    "        score = score_1\n",
    "    log4 = \"ens single, score: {0}. score1: {1}, score2: {2}\".format(score, score_1, score_2)\n",
    "\n",
    "    log = log1 + \"\\n\" + log2 + \"\\n\" + log3 + \"\\n\" + log4\n",
    "\n",
    "    file_result = dir_out + 'result_per_time_and_single' + str_type + '.txt'\n",
    "    with open(file_result, mode='w') as f:\n",
    "        f.write(log)\n",
    "    print(log)\n",
    "    # ************************************************\n",
    "    \n",
    "    # final fusion multi\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((pred_val_ens_01s, pred_val_ens_06s, pred_val_ens_12s,\n",
    "                                      pred_val_sub1, pred_val_sub2))\n",
    "\n",
    "    if str_type == \"EXP\":\n",
    "        pred_val_fusion = model_fusion.predict(stack_pred_val)\n",
    "    else:\n",
    "        pred_val_fusion = model_fusion.predict(stack_pred_val).ravel()\n",
    "\n",
    "    score_1, score_2 = eval_pred(np_val_true_01s, pred_val_fusion, str_type)\n",
    "\n",
    "    if str_type == \"EXP\":\n",
    "        score = score_1 * 0.67 + score_2 * 0.33\n",
    "    else:\n",
    "        score = score_1\n",
    "\n",
    "    log_end = \"Validation Score: {0:.4f}, score_1: {1:.4f}, score_2: {2:.4f}\".format(score, score_1, score_2)\n",
    "    print(log_end)\n",
    "\n",
    "    file_result = dir_out + 'fusion_multi_' + str_type + '.txt'\n",
    "    with open(file_result, mode='w') as f:\n",
    "        f.write(log_end)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
