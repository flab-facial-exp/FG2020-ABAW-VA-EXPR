{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "save rolled data including openface, openpose, and label\n",
    " * rolling window: 1s, 6s, 12s\n",
    " * resampling: 1 frame (overlapped)\n",
    " * creating \"Validation\" subfolder in output folder\n",
    "'''\n",
    "#from numba import jit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from statistics import mean, median,variance,stdev\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root folder\n",
    "dir_submit = str(Path().resolve())\n",
    "dir_base = str(Path(Path().resolve()).parent) + \"\\\\base_data\"\n",
    "\n",
    "# merged data folder (va, exp : validation)\n",
    "dir_data_va_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_VA\\\\Validation\\\\\"\n",
    "dir_data_exp_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_EXP\\\\Validation\\\\\"\n",
    "\n",
    "# output data folder\n",
    "dir_out_va_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_VA_roll\\\\Validation_Frame\\\\\"\n",
    "if os.path.isdir(dir_out_va_val) == False:\n",
    "    os.makedirs(dir_out_va_val)\n",
    "\n",
    "dir_out_exp_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_EXP_roll\\\\Validation_Frame\\\\\"\n",
    "if os.path.isdir(dir_out_exp_val) == False:\n",
    "    os.makedirs(dir_out_exp_val)\n",
    "\n",
    "\n",
    "# list of file path: VA validation data \n",
    "file_data_va_val = dir_data_va_val + \"*.h5\"\n",
    "files_data_va_val = [\n",
    "    filename for filename in sorted(glob.glob(file_data_va_val))\n",
    "]\n",
    "log = \"file number of files_data_va_val: {0}\".format(len(files_data_va_val))\n",
    "print(log)\n",
    "\n",
    "\n",
    "# list of file path: EXP validation data \n",
    "file_data_exp_val = dir_data_exp_val + \"*.h5\"\n",
    "files_data_exp_val = [\n",
    "    filename for filename in sorted(glob.glob(file_data_exp_val))\n",
    "]\n",
    "log = \"file number of files_data_exp_val: {0}\".format(len(files_data_exp_val))\n",
    "print(log)\n",
    "\n",
    "# rolling and resampling time list\n",
    "time_list =  [30, 180, 360, 90]  #frame\n",
    "time_shift = [ 1,   1,   1,  1]  #frame\n",
    "Th_conf = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rolled data (1s, 6s, 12s) *str_type: \"EXP\" or \"VA\"\n",
    "def data_roll_np(in_data_np, th_conf, str_type):\n",
    "    \n",
    "    # get length of columns of source data and output data\n",
    "    len_col = len(in_data_np[0,:])\n",
    "    len_col_out = len(in_data_np[0,:]) * 4\n",
    "    \n",
    "    # delete nan data\n",
    "    data = in_data_np[~np.isnan(in_data_np).any(axis=1), :]\n",
    "    \n",
    "    # cut data of that \"confidence\" is below th_conf  閾値以下のデータをカット\n",
    "    data = in_data_np[in_data_np[:,3]>=th_conf]\n",
    "    \n",
    "    # create dummy numpy array (for merge)\n",
    "    data_1s = [np.zeros(len_col_out)]\n",
    "    data_6s = [np.zeros(len_col_out)]\n",
    "    data_12s = [np.zeros(len_col_out)]\n",
    "    data_3s = [np.zeros(len_col_out)]\n",
    "    \n",
    "    if len(data) <1:\n",
    "        return data_1s,data_6s, data_12s, data_3s\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # calc loop number \n",
    "    max_count = data[:,0].max()  #math.round(data['frame'].max()).astype(int)\n",
    "    max_count = int(max_count)\n",
    "    \n",
    "    # loop of data rolling, resampling\n",
    "    for count in range(max_count):\n",
    "        # 1 sec window 1 frame shift\n",
    "        start = count * time_shift[0] - time_list[0]*0.5\n",
    "        stop = start + time_list[0]\n",
    "        # substract window sec data\n",
    "        #data_tmp = data[(data[:,2]>=start) & (data[:,2]<stop)]\n",
    "        data_tmp = data[(data[:,0]>=start) & (data[:,0]<stop)]\n",
    "        # if data_tmp is empty, return nan array\n",
    "        if len(data_tmp)<1:\n",
    "            data_mean = np.zeros(len_col)\n",
    "            data_mean[:] = np.nan\n",
    "            data_mean[0] = count + 1\n",
    "            data_mean[2] = count/30\n",
    "            data_std = np.zeros(len_col)\n",
    "            data_std[:] = np.nan\n",
    "            data_range = np.zeros(len_col)\n",
    "            data_range[:] = np.nan\n",
    "            data_slope = np.zeros(len_col)\n",
    "            data_slope[:] = np.nan\n",
    "        else:\n",
    "            # mean\n",
    "            data_mean = data_tmp.mean(axis=0)\n",
    "            # set \" timestamp\" to window average\n",
    "            data_mean[0] = count + 1\n",
    "            # std\n",
    "            data_std = data_tmp.std(axis=0)\n",
    "            # range: max-min\n",
    "            data_range = data_tmp.max(axis=0) - data_tmp.min(axis=0)\n",
    "            # slope\n",
    "            # create empty list (length is columns' length)\n",
    "            data_slope = np.empty(len_col)\n",
    "            # if data_tmp's length is over 2, calculate slope\n",
    "            if len(data_tmp) > 1:\n",
    "                for col in range(len_col):\n",
    "                    x_arr = data_tmp[:,2].ravel()\n",
    "                    y_arr = data_tmp[:,col].ravel()\n",
    "                    if (len(y_arr[~np.isnan(y_arr)])>1) & (len(x_arr[~np.isnan(x_arr)])>1):\n",
    "                        try:\n",
    "                            a, b = np.polyfit(x_arr, y_arr, 1)\n",
    "                        except:\n",
    "                            a=0\n",
    "                    else:\n",
    "                        a=0\n",
    "                    #lr.fit(x_arr, y_arr)\n",
    "                    data_slope[col] = a\n",
    "            else:\n",
    "                data_slope[:] = 0\n",
    "            # if str_type is \"EXP\", calculate the mode of expression in window\n",
    "            if str_type == \"EXP\":\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = int(data[data[:,0]==count+1,331].mean())\n",
    "                    except:\n",
    "                        data_mean[331] = -1\n",
    "            else:\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                    data_mean[332] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = data[data[:,0]==count+1,331].mean()\n",
    "                        data_mean[332] = data[data[:,0]==count+1,332].mean()\n",
    "                    except:\n",
    "                        data_mean[331] = np.nan\n",
    "                        data_mean[332] = np.nan\n",
    "        \n",
    "        # merge mean, std, range, slope\n",
    "        data_mix = np.append(data_mean, data_std)\n",
    "        data_mix = np.append(data_mix, data_range)\n",
    "        data_mix = np.append(data_mix, data_slope)\n",
    "        \n",
    "        data_1s = np.append(data_1s, [data_mix], axis=0)\n",
    "\n",
    "    #print(data_1s)\n",
    "\n",
    "    count = 0\n",
    "    #max_count = math.floor(((data[' timestamp'].max() / 3) - 1))\n",
    "    for count in range(max_count):\n",
    "        # 6 sec window 1 frame shift\n",
    "        start = count * time_shift[1] - time_list[1]*0.5\n",
    "        stop = start + time_list[1]\n",
    "        # substract window sec data\n",
    "        #data_tmp = data[(data[:,2]>=start) & (data[:,2]<stop)]\n",
    "        data_tmp = data[(data[:,0]>=start) & (data[:,0]<stop)]\n",
    "        # if data_tmp is empty, return nan array\n",
    "        if len(data_tmp)<1:\n",
    "            data_mean = np.zeros(len_col)\n",
    "            data_mean[:] = np.nan\n",
    "            data_mean[0] = count + 1\n",
    "            data_mean[2] = count/30\n",
    "            data_std = np.zeros(len_col)\n",
    "            data_std[:] = np.nan\n",
    "            data_range = np.zeros(len_col)\n",
    "            data_range[:] = np.nan\n",
    "            data_slope = np.zeros(len_col)\n",
    "            data_slope[:] = np.nan\n",
    "        else:\n",
    "            # mean\n",
    "            data_mean = data_tmp.mean(axis=0)\n",
    "            # set \" timestamp\" to window average\n",
    "            data_mean[0] = count + 1\n",
    "            # std\n",
    "            data_std = data_tmp.std(axis=0)\n",
    "            # range: max-min\n",
    "            data_range = data_tmp.max(axis=0) - data_tmp.min(axis=0)\n",
    "            # slope\n",
    "            # create empty list (length is columns' length)\n",
    "            data_slope = np.empty(len_col)\n",
    "            # if data_tmp's length is over 2, calculate slope\n",
    "            if len(data_tmp) > 1:\n",
    "                for col in range(len_col):\n",
    "                    x_arr = data_tmp[:,2].ravel()\n",
    "                    y_arr = data_tmp[:,col].ravel()\n",
    "                    if (len(y_arr[~np.isnan(y_arr)])>1) & (len(x_arr[~np.isnan(x_arr)])>1):\n",
    "                        try:\n",
    "                            a, b = np.polyfit(x_arr, y_arr, 1)\n",
    "                        except:\n",
    "                            a=0\n",
    "                    else:\n",
    "                        a=0\n",
    "                    #lr.fit(x_arr, y_arr)\n",
    "                    data_slope[col] = a\n",
    "            else:\n",
    "                data_slope[:] = 0\n",
    "            # if str_type is \"EXP\", calculate the mode of expression in window\n",
    "            if str_type == \"EXP\":\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = int(data[data[:,0]==count+1,331].mean())\n",
    "                    except:\n",
    "                        data_mean[331] = -1\n",
    "            else:\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                    data_mean[332] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = data[data[:,0]==count+1,331].mean()\n",
    "                        data_mean[332] = data[data[:,0]==count+1,332].mean()\n",
    "                    except:\n",
    "                        data_mean[331] = np.nan\n",
    "                        data_mean[332] = np.nan\n",
    "        \n",
    "        # merge mean, std, range, slope\n",
    "        data_mix = np.append(data_mean, data_std)\n",
    "        data_mix = np.append(data_mix, data_range)\n",
    "        data_mix = np.append(data_mix, data_slope)\n",
    "        \n",
    "        data_6s = np.append(data_6s, [data_mix], axis=0)\n",
    "\n",
    "    count = 0\n",
    "    #max_count = math.floor(((data[' timestamp'].max() / 3) - 1))\n",
    "    for count in range(max_count):\n",
    "        # 12 sec window 1 frame shift\n",
    "        start = count * time_shift[2] - time_list[2]*0.5\n",
    "        stop = start + time_list[2]\n",
    "        # substract window sec data\n",
    "        #data_tmp = data[(data[:,2]>=start) & (data[:,2]<stop)]\n",
    "        data_tmp = data[(data[:,0]>=start) & (data[:,0]<stop)]\n",
    "        # if data_tmp is empty, return nan array\n",
    "        if len(data_tmp)<1:\n",
    "            data_mean = np.zeros(len_col)\n",
    "            data_mean[:] = np.nan\n",
    "            data_mean[0] = count + 1\n",
    "            data_mean[2] = count/30\n",
    "            data_std = np.zeros(len_col)\n",
    "            data_std[:] = np.nan\n",
    "            data_range = np.zeros(len_col)\n",
    "            data_range[:] = np.nan\n",
    "            data_slope = np.zeros(len_col)\n",
    "            data_slope[:] = np.nan\n",
    "        else:\n",
    "            # mean\n",
    "            data_mean = data_tmp.mean(axis=0)\n",
    "            # set \" timestamp\" to window average\n",
    "            data_mean[0] = count + 1\n",
    "            # std\n",
    "            data_std = data_tmp.std(axis=0)\n",
    "            # range: max-min\n",
    "            data_range = data_tmp.max(axis=0) - data_tmp.min(axis=0)\n",
    "            # slope\n",
    "            # create empty list (length is columns' length)\n",
    "            data_slope = np.empty(len_col)\n",
    "            # if data_tmp's length is over 2, calculate slope\n",
    "            if len(data_tmp) > 1:\n",
    "                for col in range(len_col):\n",
    "                    x_arr = data_tmp[:,2].ravel()\n",
    "                    y_arr = data_tmp[:,col].ravel()\n",
    "                    if (len(y_arr[~np.isnan(y_arr)])>1) & (len(x_arr[~np.isnan(x_arr)])>1):\n",
    "                        try:\n",
    "                            a, b = np.polyfit(x_arr, y_arr, 1)\n",
    "                        except:\n",
    "                            a=0\n",
    "                    else:\n",
    "                        a=0\n",
    "                    #lr.fit(x_arr, y_arr)\n",
    "                    data_slope[col] = a\n",
    "            else:\n",
    "                data_slope[:] = 0\n",
    "            # if str_type is \"EXP\", calculate the mode of expression in window\n",
    "            if str_type == \"EXP\":\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = int(data[data[:,0]==count+1,331].mean())\n",
    "                    except:\n",
    "                        data_mean[331] = -1\n",
    "            else:\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                    data_mean[332] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = data[data[:,0]==count+1,331].mean()\n",
    "                        data_mean[332] = data[data[:,0]==count+1,332].mean()\n",
    "                    except:\n",
    "                        data_mean[331] = np.nan\n",
    "                        data_mean[332] = np.nan\n",
    "        \n",
    "        # merge mean, std, range, slope\n",
    "        data_mix = np.append(data_mean, data_std)\n",
    "        data_mix = np.append(data_mix, data_range)\n",
    "        data_mix = np.append(data_mix, data_slope)\n",
    "        \n",
    "        data_12s = np.append(data_12s, [data_mix], axis=0)\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    #max_count = math.floor(((data[' timestamp'].max() / 3) - 1)) *********************************\n",
    "    for count in range(max_count):\n",
    "        # 03 sec window 1 frame shift\n",
    "        start = count * time_shift[3] - time_list[3]*0.5\n",
    "        stop = start + time_list[3]\n",
    "        # substract window sec data\n",
    "        #data_tmp = data[(data[:,2]>=start) & (data[:,2]<stop)]\n",
    "        data_tmp = data[(data[:,0]>=start) & (data[:,0]<stop)]\n",
    "        # if data_tmp is empty, return nan array\n",
    "        if len(data_tmp)<1:\n",
    "            data_mean = np.zeros(len_col)\n",
    "            data_mean[:] = np.nan\n",
    "            data_mean[0] = count + 1\n",
    "            data_mean[2] = count/30\n",
    "            data_std = np.zeros(len_col)\n",
    "            data_std[:] = np.nan\n",
    "            data_range = np.zeros(len_col)\n",
    "            data_range[:] = np.nan\n",
    "            data_slope = np.zeros(len_col)\n",
    "            data_slope[:] = np.nan\n",
    "        else:\n",
    "            # mean\n",
    "            data_mean = data_tmp.mean(axis=0)\n",
    "            # set \" timestamp\" to window average\n",
    "            data_mean[0] = count + 1\n",
    "            # std\n",
    "            data_std = data_tmp.std(axis=0)\n",
    "            # range: max-min\n",
    "            data_range = data_tmp.max(axis=0) - data_tmp.min(axis=0)\n",
    "            # slope\n",
    "            # create empty list (length is columns' length)\n",
    "            data_slope = np.empty(len_col)\n",
    "            # if data_tmp's length is over 2, calculate slope\n",
    "            if len(data_tmp) > 1:\n",
    "                for col in range(len_col):\n",
    "                    x_arr = data_tmp[:,2].ravel()\n",
    "                    y_arr = data_tmp[:,col].ravel()\n",
    "                    if (len(y_arr[~np.isnan(y_arr)])>1) & (len(x_arr[~np.isnan(x_arr)])>1):\n",
    "                        try:\n",
    "                            a, b = np.polyfit(x_arr, y_arr, 1)\n",
    "                        except:\n",
    "                            a=0\n",
    "                    else:\n",
    "                        a=0\n",
    "                    #lr.fit(x_arr, y_arr)\n",
    "                    data_slope[col] = a\n",
    "            else:\n",
    "                data_slope[:] = 0\n",
    "            # if str_type is \"EXP\", calculate the mode of expression in window\n",
    "            if str_type == \"EXP\":\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = int(data[data[:,0]==count+1,331].mean())\n",
    "                    except:\n",
    "                        data_mean[331] = -1\n",
    "            else:\n",
    "                if len(data[data[:,0]==count+1,331])<1:\n",
    "                    data_mean[331] = np.nan\n",
    "                    data_mean[332] = np.nan\n",
    "                else:\n",
    "                    try:\n",
    "                        data_mean[331] = data[data[:,0]==count+1,331].mean()\n",
    "                        data_mean[332] = data[data[:,0]==count+1,332].mean()\n",
    "                    except:\n",
    "                        data_mean[331] = np.nan\n",
    "                        data_mean[332] = np.nan\n",
    "        \n",
    "        # merge mean, std, range, slope\n",
    "        data_mix = np.append(data_mean, data_std)\n",
    "        data_mix = np.append(data_mix, data_range)\n",
    "        data_mix = np.append(data_mix, data_slope)\n",
    "        \n",
    "        data_3s = np.append(data_3s, [data_mix], axis=0)\n",
    "    \n",
    "    \n",
    "    # delete first data (dummy data)\n",
    "    data_1s = np.delete(data_1s, 0, 0) #data_1s.reset_index(drop=True)\n",
    "    data_6s = np.delete(data_6s, 0, 0) #data_6s.reset_index(drop=True)\n",
    "    data_12s = np.delete(data_12s, 0, 0) #data_12s.reset_index(drop=True)\n",
    "    \n",
    "    data_3s = np.delete(data_3s, 0, 0) #data_3s.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    log = \"shape 1s: {0}, shape 6s: {1}, shape 12s: {2}, shape 3s: {3}\".format(data_1s.shape, data_6s.shape, \n",
    "                                                                               data_12s.shape, data_3s.shape)\n",
    "    print(log)\n",
    "    \n",
    "    \n",
    "    return data_1s, data_6s, data_12s, data_3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential run [data rolling]\n",
    "def seq_data_rolling(files_data, dir_out, th_conf, str_type):\n",
    "\n",
    "    count = 1\n",
    "    max_count = len(files_data)\n",
    "    \n",
    "    for f_data in files_data:\n",
    "        name_data = os.path.splitext(os.path.basename(f_data))[0]\n",
    "        \n",
    "        # read data to dataframe\n",
    "        #data_tmp = pd.read_csv(f_data)\n",
    "        data_tmp = pd.read_hdf(f_data, 'key')\n",
    "        \n",
    "        data_tmp.columns = data_tmp.columns.astype(str)\n",
    "        \n",
    "        # if str_type is \"EXP\", delete nan label columns\n",
    "        if str_type == \"EXP\":\n",
    "            data_tmp = data_tmp.drop([\"Anger\",\"Disgust\",\"Fear\",\"Happiness\",\"Sadness\",\"Surprise\"],axis=1)\n",
    "\n",
    "        # create average, std, rang, sope columns\n",
    "        col_avg = data_tmp.columns + \"-avg\"\n",
    "        col_std = data_tmp.columns + \"-std\"\n",
    "        col_range = data_tmp.columns + \"-range\"\n",
    "        col_slope = data_tmp.columns + \"-slope\"\n",
    "        col_list = col_avg.append(col_std)\n",
    "        col_list = col_list.append(col_range)\n",
    "        col_list = col_list.append(col_slope)\n",
    "\n",
    "        # convet pandas data to numpy array\n",
    "        data_arr = data_tmp.values\n",
    "        \n",
    "        # calc rolling\n",
    "        data1, data6, data12, data3 = data_roll_np(data_arr,  th_conf, str_type)\n",
    "        \n",
    "        \n",
    "        # convert numpy array to pandas data\n",
    "        data_1s = pd.DataFrame(data1, columns = col_list)\n",
    "        data_6s = pd.DataFrame(data6, columns = col_list)\n",
    "        data_12s = pd.DataFrame(data12, columns = col_list)\n",
    "        data_3s = pd.DataFrame(data3, columns = col_list)\n",
    "        \n",
    "        # save windowed data\n",
    "        file_out_1s = dir_out + name_data + '_' + str(1).zfill(2) + 's.h5'\n",
    "        #data_1s.to_csv(file_out_1s, index=False, float_format='%.6g')\n",
    "        data_1s.to_hdf(file_out_1s, \"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "        \n",
    "        file_out_6s = dir_out + name_data + '_' + str(6).zfill(2) + 's.h5'\n",
    "        #data_6s.to_csv(file_out_6s, index=False, float_format='%.6g')\n",
    "        data_6s.to_hdf(file_out_6s, \"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "        \n",
    "        file_out_12s = dir_out + name_data + '_' + str(12).zfill(2) + 's.h5'\n",
    "        #data_12s.to_csv(file_out_12s, index=False, float_format='%.6g')\n",
    "        data_12s.to_hdf(file_out_12s, \"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "        \n",
    "        file_out_3s = dir_out + name_data + '_' + str(3).zfill(2) + 's.h5'\n",
    "        #data_12s.to_csv(file_out_12s, index=False, float_format='%.6g')\n",
    "        data_3s.to_hdf(file_out_3s, \"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "\n",
    "        log = \"{0}/{1}, {2}\".format(count, max_count, name_data)\n",
    "        print(log)\n",
    "        count = count + 1\n",
    "\n",
    "    print('**** finished ****') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# roll and resample data: VA validation\n",
    "seq_data_rolling(files_data_va_val, dir_out_va_val, Th_conf, \"VA\")\n",
    "\n",
    "# roll and resample data: EXP validatio\n",
    "seq_data_rolling(files_data_exp_val, dir_out_exp_val, Th_conf, \"EXP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
