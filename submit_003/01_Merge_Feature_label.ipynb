{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "merge openface, openpose, resnet, and label data per frame\n",
    " * need openface, openpose, resnet data\n",
    " * output folder: ./proc_data/Merged_VA, ./proc_data/Merged_EXP\n",
    "   * create \"Training\", \"Validation\" sub-folder in output folder\n",
    " * filename of openface: <vidoe name>.csv\n",
    " * filename of openpose: <vidoe name>_openpose.csv\n",
    " * filename of resnet: <vidoe name>_resnet50.h5\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root folder\n",
    "dir_submit = str(Path().resolve())\n",
    "dir_base = str(Path(Path().resolve()).parent) + \"\\\\base_data\"\n",
    "\n",
    "# openface, openpose, resnet folder\n",
    "dir_of = dir_base + \"\\\\OpenFace\\\\\"\n",
    "dir_op = dir_base + \"\\\\OpenPose\\\\\"\n",
    "dir_rn = dir_base + \"\\\\Resnet\\\\\"\n",
    "\n",
    "# VA, EXP label folder\n",
    "dir_lva = dir_base + \"\\\\annotations\\\\VA_Set\\\\\"\n",
    "dir_lexp = dir_base + \"\\\\annotations\\\\EXPR_Set\\\\\"\n",
    "\n",
    "# VA, EXP output dataset folder\n",
    "dir_out_va_train = dir_base + \"\\\\Merged_with_resnet\\\\Merged_VA\\\\Training\\\\\"\n",
    "\n",
    "if os.path.isdir(dir_out_va_train) == False:\n",
    "    os.makedirs(dir_out_va_train)\n",
    "dir_out_va_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_VA\\\\Validation\\\\\"\n",
    "if os.path.isdir(dir_out_va_val) == False:\n",
    "    os.makedirs(dir_out_va_val)\n",
    "\n",
    "dir_out_exp_train = dir_base + \"\\\\Merged_with_resnet\\\\Merged_EXP\\\\Training\\\\\"\n",
    "if os.path.isdir(dir_out_exp_train) == False:\n",
    "    os.makedirs(dir_out_exp_train)\n",
    "dir_out_exp_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_EXP\\\\Validation\\\\\"\n",
    "if os.path.isdir(dir_out_exp_val) == False:\n",
    "    os.makedirs(dir_out_exp_val)\n",
    "\n",
    "# create standardization parameter folder\n",
    "dir_norm_param = dir_base + \"\\\\Merged_with_resnet\\\\Norm\\\\\"\n",
    "if os.path.isdir(dir_norm_param) == False:\n",
    "    os.makedirs(dir_norm_param)\n",
    "\n",
    "# standardize data  or not\n",
    "flag_norm = True\n",
    "\n",
    "# exclude file name (with out \"file_exc\") *exclude multi-person\n",
    "file_exc = [\"6-30-1920x1080\", \"10-60-1280x720\", \"30-30-1920x1080\", \"46-30-484x360\", \"52-30-1280x720\",\n",
    "            \"130-25-1280x720\", \"135-24-1920x1080\", \"video2\", \"video5\", \"video49\", \"video55\", \"video59\", \"video74\"\n",
    "           ]\n",
    "\n",
    "# openface file serach name\n",
    "file_of = dir_of + \"*.csv\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_of))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if name == j:\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_of = files_tmp\n",
    "log = \"file number of openface: {0}\".format(len(files_of))\n",
    "print(log)\n",
    "\n",
    "# openpose file serach name\n",
    "file_op = dir_op + \"*.csv\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_op))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if name == j + \"_openpose\":\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_op = files_tmp\n",
    "log = \"file number of openpose: {0}\".format(len(files_op))\n",
    "print(log)\n",
    "\n",
    "\n",
    "# resnet file serach name\n",
    "file_rn = dir_rn + \"*.h5\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_rn))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if name == j + \"_resnet50\":\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_rn = files_tmp\n",
    "log = \"file number of resnet: {0}\".format(len(files_rn))\n",
    "print(log)\n",
    "\n",
    "# VA label file serach name *Train*\n",
    "file_lva_train = dir_lva + \"Training_Set\\\\*.txt\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_lva_train))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if (name == j) | (name == j + \"_right\") | (name == j + \"_left\"):\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_lva_train = files_tmp\n",
    "log = \"file number of VA label (Train): {0}\".format(len(files_lva_train))\n",
    "print(log)\n",
    "\n",
    "# VA label file serach name *Validation*\n",
    "file_lva_val = dir_lva + \"Validation_Set\\\\*.txt\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_lva_val))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if (name == j) | (name == j + \"_right\") | (name == j + \"_left\"):\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_lva_val = files_tmp\n",
    "log = \"file number of VA label (Validation): {0}\".format(len(files_lva_val))\n",
    "print(log)\n",
    "\n",
    "# EXP label file serach name *Train*\n",
    "file_lexp_train = dir_lexp + \"Training_Set\\\\*.txt\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_lexp_train))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if (name == j) | (name == j + \"_right\") | (name == j + \"_left\"):\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_lexp_train = files_tmp\n",
    "log = \"file number of EXP label (Train): {0}\".format(len(files_lexp_train))\n",
    "print(log)\n",
    "\n",
    "# EXP label file serach name *Validation*\n",
    "file_lexp_val = dir_lexp + \"Validation_Set\\\\*.txt\"\n",
    "files_tmp = [\n",
    "    filename for filename in sorted(glob.glob(file_lexp_val))\n",
    "]\n",
    "for i in files_tmp:\n",
    "    name = os.path.splitext(os.path.basename(i))[0]\n",
    "    for j in file_exc:\n",
    "        if (name == j) | (name == j + \"_right\") | (name == j + \"_left\"):\n",
    "            files_tmp.remove(i)\n",
    "            break\n",
    "files_lexp_val = files_tmp\n",
    "log = \"file number of EXP label (Validation): {0}\".format(len(files_lexp_val))\n",
    "print(log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standardization parameter\n",
    "def get_standardize_param(in_data):\n",
    "    # standardize\n",
    "    data_m = in_data.mean()\n",
    "    data_s = in_data.std()\n",
    "\n",
    "    return data_m, data_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standardization parameter from files (ex. openface files) and save paramter\n",
    "# save path is .h5 file\n",
    "def standardize_data(files_data, out_path_mean, out_path_std):\n",
    "    # make columns\n",
    "    data_tmp = pd.read_csv(files_data[0])\n",
    "    data_columns = data_tmp.columns\n",
    "    \n",
    "    # create empty\n",
    "    np_data = np.zeros((1,len(data_columns)))\n",
    "    # set loop\n",
    "    count = 1\n",
    "    max_count = len(files_data)\n",
    "    \n",
    "    for i in range(max_count):\n",
    "        data = pd.read_csv(files_data[i])\n",
    "        if len(data) < 1:\n",
    "            count = count+1\n",
    "            continue\n",
    "        np_data_tmp = data.values\n",
    "        \n",
    "        np_data = np.append(np_data, np_data_tmp, axis=0)\n",
    "        \n",
    "        log = \"{0}/{1}, data shape: {2}, sum: {3}\".format(count, max_count,\n",
    "                                                          np_data_tmp.shape, np_data.shape)\n",
    "        print(log)\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "    np_data = np.delete(np_data, 0, 0)\n",
    "    data = pd.DataFrame(np_data)\n",
    "    data.columns = data_columns\n",
    "    \n",
    "    log = \"all loaded, data shape: {2}\".format(count, max_count, np_data.shape)\n",
    "    print(log)\n",
    "\n",
    "    pmean, pstd = get_standardize_param(data)\n",
    "\n",
    "    #pmean.to_csv(out_path_mean)\n",
    "    #pstd.to_csv(out_path_std)\n",
    "    pmean.to_hdf(out_path_mean, key=\"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "    pstd.to_hdf(out_path_std, key=\"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "    \n",
    "    return pmean, pstd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standardization parameter from files (ex. openface files) and save paramter\n",
    "# save path is .h5 file\n",
    "def standardize_data_rn(files_data, out_path_mean, out_path_std):\n",
    "    # make columns\n",
    "    #data_tmp = pd.read_csv(files_data[0])\n",
    "    data_tmp = pd.read_hdf(files_data[0])\n",
    "    data_tmp = data_tmp.reset_index(drop=True).iloc[0:10,0:201]\n",
    "    data_columns = data_tmp.columns\n",
    "    #print(data_columns)\n",
    "    #print(data_tmp)\n",
    "    \n",
    "    # create empty\n",
    "    np_data = np.zeros((1,len(data_columns)))\n",
    "    # set loop\n",
    "    count = 1\n",
    "    max_count = len(files_data)\n",
    "    \n",
    "    for i in range(max_count):\n",
    "        #data = pd.read_csv(files_data[i])\n",
    "        data = pd.read_hdf(files_data[i])\n",
    "        data = data.reset_index(drop=True).iloc[:,0:201]\n",
    "        #print(data)\n",
    "        if len(data) < 1:\n",
    "            count = count+1\n",
    "            continue\n",
    "        np_data_tmp = data.values\n",
    "        \n",
    "        np_data = np.append(np_data, np_data_tmp, axis=0)\n",
    "        \n",
    "        log = \"{0}/{1}, data shape: {2}, sum: {3}\".format(count, max_count,\n",
    "                                                          np_data_tmp.shape, np_data.shape)\n",
    "        print(log)\n",
    "        \n",
    "        count = count + 1\n",
    "\n",
    "    np_data = np.delete(np_data, 0, 0)\n",
    "    data = pd.DataFrame(np_data)\n",
    "    data.columns = data_columns\n",
    "    \n",
    "    log = \"all loaded, data shape: {2}\".format(count, max_count, np_data.shape)\n",
    "    print(log)\n",
    "\n",
    "    pmean, pstd = get_standardize_param(data)\n",
    "\n",
    "    #pmean.to_csv(out_path_mean)\n",
    "    #pstd.to_csv(out_path_std)\n",
    "    pmean.to_hdf(out_path_mean, key=\"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "    pstd.to_hdf(out_path_std, key=\"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "    \n",
    "    return pmean, pstd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if flag = True, get standardization parameter of openface, openpose, resnet\n",
    "# calculate and save parameter\n",
    "if flag_norm == True:\n",
    "    # openface standardization \n",
    "    mean_data_of, std_data_of = standardize_data(files_of,dir_norm_param + \"raw_mean_of.h5\",\n",
    "                                       dir_norm_param + \"raw_std_of.h5\")\n",
    "    # openpose standardization \n",
    "    mean_data_op, std_data_op = standardize_data(files_op,dir_norm_param + \"raw_mean_op.h5\",\n",
    "                                       dir_norm_param + \"raw_std_op.h5\")\n",
    "    # resnet standardization \n",
    "    mean_data_rn, std_data_rn = standardize_data_rn(files_rn,dir_norm_param + \"raw_mean_rn.h5\",\n",
    "                                       dir_norm_param + \"raw_std_rn.h5\")\n",
    "    # merge\n",
    "    mean_data = mean_data_of.append(mean_data_op)\n",
    "    mean_data = mean_data.append(mean_data_rn)\n",
    "    mean_data = mean_data.reset_index()\n",
    "    std_data  = std_data_of.append(std_data_op)\n",
    "    std_data  = std_data.append(std_data_rn)\n",
    "    std_data = std_data.reset_index()\n",
    "else:\n",
    "    mean_data = 1\n",
    "    std_data = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load standardized parameter\n",
    "# openface standardization \n",
    "mean_data_of = pd.read_hdf(dir_norm_param + \"raw_mean_of.h5\", key=\"key\")\n",
    "std_data_of = pd.read_hdf(dir_norm_param + \"raw_std_of.h5\", key=\"key\")\n",
    "# openpose standardization \n",
    "mean_data_op = pd.read_hdf(dir_norm_param + \"raw_mean_op.h5\", key=\"key\")\n",
    "std_data_op = pd.read_hdf(dir_norm_param + \"raw_std_op.h5\", key=\"key\")\n",
    "# resnet standardization \n",
    "mean_data_rn = pd.read_hdf(dir_norm_param + \"raw_mean_rn.h5\", key=\"key\")\n",
    "std_data_rn = pd.read_hdf(dir_norm_param + \"raw_std_rn.h5\", key=\"key\")\n",
    "\n",
    "# merge\n",
    "mean_data = mean_data_of.append(mean_data_op)\n",
    "mean_data = mean_data.append(mean_data_rn)\n",
    "mean_data = mean_data.reset_index()\n",
    "std_data  = std_data_of.append(std_data_op)\n",
    "std_data  = std_data.append(std_data_rn)\n",
    "std_data = std_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge openface, openpose and label\n",
    "def merge_of_data_label(files_of, dir_op, dir_rn, files_label, dir_out, param_mean, param_std, flag_norm, str_type):\n",
    "    count = 1\n",
    "    max_count = len(files_label)\n",
    "    if len(files_of) <1:\n",
    "        print(\"openface files are not found\")\n",
    "        data_merge = pd.DataFrame()\n",
    "        return\n",
    "    \n",
    "    # set label file\n",
    "    for f_lv in files_label:\n",
    "        # get label name\n",
    "        name_lv = os.path.splitext(os.path.basename(f_lv))[0]\n",
    "        \n",
    "        for i in range(len(files_of)):\n",
    "            # get openface name\n",
    "            name_of = os.path.splitext(os.path.basename(files_of[i]))[0]\n",
    "            if name_lv != name_of:\n",
    "                continue\n",
    "            else:\n",
    "                # set save file mame\n",
    "                file_out = dir_out + name_of + \".h5\"\n",
    "\n",
    "                # read label, set frame column\n",
    "                data_lv = pd.read_csv(f_lv)\n",
    "                data_lv['frame'] = data_lv.index+1\n",
    "                \n",
    "                # read openface data, delete duplicated frame, set index based on \"frame\"\n",
    "                # openface ** \n",
    "                data_of = pd.read_csv(files_of[i])\n",
    "                data_of = data_of.drop_duplicates([\"frame\"])\n",
    "                data_of = data_of.set_index(\"frame\", drop=False)\n",
    "                \n",
    "                # openpose ** \n",
    "                # read openpose data, delete duplicated frame, set frame column based on \"Unnamed: 0\"+1\n",
    "                f_op = dir_op + name_of + \"_openpose.csv\"\n",
    "                data_op = pd.read_csv(f_op)\n",
    "                #data_op = data_op.drop_duplicates([\"Unnamed: 0\"])\n",
    "                data_op[\"frame\"] = data_op[\"Unnamed: 0\"]+1\n",
    "                data_op = data_op.set_index(\"frame\")\n",
    "                \n",
    "                # resnet ** \n",
    "                f_rn = dir_rn + name_of + \"_resnet50.h5\"\n",
    "                data_rn = pd.read_hdf(f_rn).iloc[:,0:201]\n",
    "                #data_rn = data_rn.drop_duplicates([\"Unnamed: 0\"])\n",
    "                #data_rn[\"frame\"] = data_op[\"Unnamed: 0\"]+1\n",
    "                data_rn = data_rn.set_index(\"frame\")\n",
    "                \n",
    "                # join data openface, openpose\n",
    "                data_tmp = data_of.join(data_op)\n",
    "                data_tmp = data_tmp.join(data_rn)\n",
    "                data_tmp = data_tmp.fillna(0)\n",
    "                data_tmp = data_tmp[data_tmp[\"frame\"]>0]\n",
    "                data_tmp = data_tmp.reset_index(drop=True)\n",
    "                \n",
    "                #print(data_tmp.shape)\n",
    "                \n",
    "                # if flag = True, \n",
    "                if flag_norm == True:\n",
    "                    # standardize *** \n",
    "                    col_len = len(data_tmp.columns)\n",
    "                    for col in range(col_len):\n",
    "                        if (col >= 5) & (col <= 35):\n",
    "                            data_tmp.iloc[:,col] = (data_tmp.iloc[:,col] - param_mean.iloc[col,1]) / param_std.iloc[col,1]\n",
    "                        elif (col >= 56) & (col <= 130):\n",
    "                            data_tmp.iloc[:,col] = (data_tmp.iloc[:,col] - param_mean.iloc[col,1]) / param_std.iloc[col,1]\n",
    "                        elif (col >= 131):\n",
    "                            data_tmp.iloc[:,col] = (data_tmp.iloc[:,col] - param_mean.iloc[col,1]) / param_std.iloc[col,1]\n",
    "                    #data_of = (data_of - data_of.mean()) / data_x.std()\n",
    "\n",
    "                # merge data and label based on \"frame\"\n",
    "                data_merge = data_tmp.merge(data_lv, on='frame', how='left')\n",
    "                \n",
    "                name_op = os.path.splitext(os.path.basename(f_op))[0].replace(\"_openpose\", \"\")\n",
    "                name_rn = os.path.splitext(os.path.basename(f_rn))[0].replace(\"_resnet50\", \"\")\n",
    "                # save merged file\n",
    "                #data_merge.to_csv(file_out, index=False, float_format='%.6g')\n",
    "                data_merge.to_hdf(file_out, key=\"key\", mode=\"w\", complevel=5, complib=\"lzo\")\n",
    "                log = \"{0}/{1}: {2}, {3}, {4}\".format(count, max_count, name_of, name_op, name_rn)\n",
    "                print(log)\n",
    "                count = count + 1\n",
    "                break\n",
    "    log = \"** finished **\"\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create and save merge data \"VA Training\"\n",
    "merge_of_data_label(files_of, dir_op, dir_rn, files_lva_train, dir_out_va_train,\n",
    "                   mean_data, std_data, flag_norm, \"VA\")\n",
    "\n",
    "# create and save merge data \"VA Validation\"\n",
    "merge_of_data_label(files_of, dir_op, dir_rn, files_lva_val, dir_out_va_val,\n",
    "                   mean_data, std_data, flag_norm, \"VA\")\n",
    "\n",
    "# create and save merge data \"EXP Training\"\n",
    "merge_of_data_label(files_of, dir_op, dir_rn, files_lexp_train, dir_out_exp_train,\n",
    "                   mean_data, std_data, flag_norm, \"EXP\")\n",
    "\n",
    "# create and save merge data \"EXP Validation\"\n",
    "merge_of_data_label(files_of, dir_op, dir_rn, files_lexp_val, dir_out_exp_val,\n",
    "                   mean_data, std_data, flag_norm, \"EXP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
