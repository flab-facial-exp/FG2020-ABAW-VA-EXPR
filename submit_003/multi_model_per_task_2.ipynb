{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create and evaluate multi time-window model taht is fusion model of multi target models\n",
    " * select type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * select sub1, sub2 type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * create \"fusion\" model\n",
    " * evaluate validation *note* labels are averaged in 1sec-window\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import sklearn #機械学習のライブラリ\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median,variance,stdev\n",
    "import math\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base dataset: concat au csv & add label, file count & drop unnecessary columns\n",
    "#   str_substract_time: ex. '02s' , str_search_key: ex. '(Subject_).*.csv'\n",
    "#   cut_start: trim data (X sec from start), cut_end: trim data (X sec from end)\n",
    "def crate_base_data(data_file_names, str_type, str_time):\n",
    "    # create empty dataframe (base dataframe)\n",
    "    #data = pd.DataFrame()\n",
    "    count = 0\n",
    "    max_count = len(data_file_names)\n",
    "    \n",
    "    data_list = [pd.DataFrame()] # <- dummy\n",
    "    \n",
    "    for  data_file in data_file_names:\n",
    "        # read au csv\n",
    "        if os.path.isfile(data_file) and os.path.getsize(data_file) > 32:\n",
    "            #print(os.path.getsize(data_file))\n",
    "            #data_tmp = pd.read_csv(data_file)\n",
    "            data_tmp = pd.read_hdf(data_file)\n",
    "        else:\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        if (len(data_tmp)<1):\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        # create column - 'count', 'Label', 'subject' (default: 0)\n",
    "        data_tmp[\"count\"] = 0\n",
    "        data_tmp[\"subject\"] = \"sample\"\n",
    "\n",
    "        # convert filename to 'subject'\n",
    "        name_train = os.path.splitext(os.path.basename(data_file))[0].replace(str_time,'')\n",
    "        #print(name_train)\n",
    "\n",
    "        #print(data_temp)\n",
    "        # get and set Label value\n",
    "        data_tmp[\"count\"]  = count\n",
    "        data_tmp[\"subject\"] = name_train\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        # ' frame-avg',' face_id-avg,' timestamp-avg',' confidence-avg,' success-avg','frame-std',' face_id-std',' confidence-std',' success-std'\n",
    "        data_tmp = data_tmp.drop(['frame-avg',' face_id-avg',' timestamp-avg',' confidence-avg',' success-avg',\n",
    "                                  'frame-std',' face_id-std',' timestamp-std',' confidence-std',' success-std',\n",
    "                                  'frame-range', ' face_id-range', ' timestamp-range', ' confidence-range', ' success-range',\n",
    "                                  'frame-slope', ' face_id-slope', ' timestamp-slope', ' confidence-slope', ' success-slope',\n",
    "                                  'Unnamed: 0-avg', 'Unnamed: 0-std', 'Unnamed: 0-range', 'Unnamed: 0-slope'\n",
    "                               ], axis=1)\n",
    "        if str_type == \"EXP\":\n",
    "            data_tmp = data_tmp.drop(['Neutral-std','Neutral-range','Neutral-slope'], axis=1)\n",
    "        else:\n",
    "            data_tmp = data_tmp.drop(['arousal-std', 'arousal-range', 'arousal-slope', \n",
    "                                     'valence-std', 'valence-range', 'valence-slope'], axis=1)\n",
    "\n",
    "        # append created data to base dataframe\n",
    "        #data = data.append(data_tmp)\n",
    "        data_list.append(data_tmp)\n",
    "\n",
    "        log = 'count: {0}, name: {1}, data shape: {2}'.format(count, name_train, data_tmp.shape)\n",
    "        print(log)\n",
    "        count = count + 1\n",
    "    # finish\n",
    "    del data_list[0]\n",
    "    data = pd.concat([x for x in data_list])\n",
    "    \n",
    "    log = '**** finished creating base dataset, data shape: {0}'.format(data.shape)\n",
    "    print(log)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balance_multi_term(data, str_type):\n",
    "    \"\"\"\n",
    "    EXP:  0,  1,  2,  3,  4,  5,  6\n",
    "        0.5,  4, 11,  8,  1,  1,  3\n",
    "    VA:\n",
    "    A|V->        -1                                +1\n",
    "    +1.00~+0.75:  1,  1,   3,  15,  10,   3,   1,   1\n",
    "    +0.75~+0.50:  1,  1,   1,   2,   1,   1,   1,   1\n",
    "    +0.50~+0.25:  5,  1,   1,   2,   1,   1,   1,   5\n",
    "    +0.25~+0.00: 10,  5,   1,   1, 0.5,   1,   5,  20\n",
    "    +0.00~-0.25:  1, 30,  10,   2,   1,  10,   1,   1\n",
    "    -0.25~-0.50:  1, 20,  40,   2,   1,  20,   1,   1\n",
    "    -0.50~-0.75:  1,  1,   1,   2,   2,   1,   1,   1\n",
    "    -0.75~-1.00:  1,  1,   1,   1,   1,   1,   1,   1\n",
    "    \"\"\"\n",
    "    \n",
    "    if str_type == \"EXP\":\n",
    "        data_list = [pd.DataFrame()]*7 # <- dummy\n",
    "        arr = [0.5,  4, 11,  8,  1,  1,  3]\n",
    "        for i in range(7):\n",
    "            data_list[i] = data.loc[data[\"Neutral-avg_01s\"]==i]\n",
    "            if arr[i] < 1:\n",
    "                data_list[i] = data_list[i][::2]\n",
    "            elif arr[i]>=2:                \n",
    "                data_list[i] = data_list[i].append([data_list[i]]*arr[i],ignore_index=True)\n",
    "                \"\"\"\n",
    "                for n in range(arr[i]):\n",
    "                    df_tmp1 = data_list[i].shift(n+1).dropna(how='any')\n",
    "                    df_tmp1.reset_index(drop = True, inplace = True)\n",
    "                    df_tmp2 = data_list[i].shift(-(n+1)).dropna(how='any')\n",
    "                    df_tmp2.reset_index(drop = True, inplace = True)\n",
    "                    df_tmp = (df_tmp1 + df_tmp2)/2\n",
    "                    df_tmp[\"Neutral-avg\"] = i\n",
    "                    data_list[i] = data_list[i].append(df_tmp, ignore_index=True)\n",
    "                \"\"\"\n",
    "        #del data_list[0]\n",
    "        out_data = pd.concat([x for x in data_list])\n",
    "    \n",
    "    else:\n",
    "        data_list = [pd.DataFrame()]*64 # <- dummy\n",
    "        arr = [1,  1,   3,  15,  10,   3,   1,   1,\n",
    "               1,  1,   1,   2,   1,   1,   1,   1,\n",
    "               5,  1,   1,   2,   1,   1,   1,   5,\n",
    "               10,  5,   1,   1, 0.5,   1,   5,  20,\n",
    "               1, 30,  10,   2,   1,  10,   1,   1,\n",
    "               1, 20,  40,   2,   1,  20,   1,   1,\n",
    "               1,  1,   1,   2,   2,   1,   1,   1,\n",
    "               1,  1,   1,   1,   1,   1,   1,   1]\n",
    "        for aro in range(8):\n",
    "            for val in range(8):\n",
    "                i = aro*8 + val\n",
    "                start_a = 1-(aro*0.25)-0.25\n",
    "                stop_a = 1-(aro*0.25)\n",
    "                start_v = val*0.25-1\n",
    "                stop_v = val*0.25-0.75\n",
    "                data_list[i] = data.loc[(data[\"valence-avg_01s\"]>=start_v)&(data[\"valence-avg_01s\"]<=stop_v)&\n",
    "                                        (data[\"arousal-avg_01s\"]>=start_a)&(data[\"arousal-avg_01s\"]<=stop_a)]\n",
    "                if arr[i] < 1:\n",
    "                    data_list[i] = data_list[i][::2]\n",
    "                elif arr[i]>=2:\n",
    "                    data_list[i] = data_list[i].append([data_list[i]]*arr[i],ignore_index=True)\n",
    "                    \"\"\"\n",
    "                    for n in range(arr[i]):\n",
    "                        df_tmp1 = data_list[i].shift(n+1).dropna(how='any')\n",
    "                        df_tmp1.reset_index(drop = True, inplace = True)\n",
    "                        df_tmp2 = data_list[i].shift(-(n+1)).dropna(how='any')\n",
    "                        df_tmp2.reset_index(drop = True, inplace = True)\n",
    "                        df_tmp = (df_tmp1 + df_tmp2)/2\n",
    "                        #print(len(df_tmp))\n",
    "                        data_list[i] = data_list[i].append(df_tmp, ignore_index=True)\n",
    "                    \"\"\"\n",
    "        out_data = pd.concat([x for x in data_list])\n",
    "        \n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_model):\n",
    "    with open(file_model, mode='rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split base data to <au>, <gaze and pose>, <eye_landmark, 2d landmark, 3d landmark>\n",
    "# ** 'count','label','subject' is contained in all splits\n",
    "def split_data(in_data):\n",
    "    # au data\n",
    "    df_au = in_data.loc[:, in_data.columns.str.contains(\"AU|count|subject|Neutral|valence|arousal\") ]\n",
    "    #df_au = df_au.join(df_lable)\n",
    "    print(\"AU data shape: \",df_au.shape)\n",
    "\n",
    "    # gaze and pose data **** temp pose\n",
    "    df_pose = in_data.loc[:, in_data.columns.str.contains(\"pose_|count|subject|Neutral|valence|arousal\") ]\n",
    "    #df_pose = df_pose.join(df_lable)\n",
    "    print(\"Gaze & Pose data shape: \",df_pose.shape)\n",
    "    \n",
    "    # eye_landmark, 2d landmark, 3d landmark data **** temp gaze\n",
    "    df_lmk = in_data.loc[:, in_data.columns.str.contains(\"gaze|count|subject|Neutral|valence|arousal\")]\n",
    "    #df_lmk = df_lmk.join(df_lable)\n",
    "    print(\"Landmark data shape: \",df_lmk.shape)\n",
    "    \n",
    "    # openpose\n",
    "    #df_op = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze\")]\n",
    "    df_op = in_data.loc[:, in_data.columns.str.contains(\"hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c|count|subject|Neutral|valence|arousal\")]\n",
    "    print(\"Opepose data shape: \",df_op.shape)\n",
    "    \n",
    "    # resnet\n",
    "    df_rn = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze|hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c\")]\n",
    "    print(\"Resnet data shape: \",df_rn.shape)\n",
    "    \n",
    "    print(\"** end **\")\n",
    "    return df_au,df_pose,df_lmk,df_op, df_rn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for single time analysis (Light GBM ...)\n",
    "def make_dataset_for_gbm(in_data, str_type):\n",
    "    \n",
    "    if str_type == \"EXP\":\n",
    "        in_data = in_data[in_data[\"Neutral-avg\"]>=0]\n",
    "        # EXP\n",
    "        # spplit features , labels\n",
    "        data_y = in_data.loc[:,[\"count\", \"subject\", \"Neutral-avg\"]]\n",
    "        data_x = in_data.drop([\"count\", \"subject\", \"Neutral-avg\"], axis=1)\n",
    "    else:\n",
    "        # VA\n",
    "        # spplit features , labels\n",
    "        data_y = in_data.loc[:,[\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"]]\n",
    "        data_x = in_data.drop([\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"], axis=1)\n",
    "    \n",
    "    dim = len(data_x.columns)\n",
    "    \n",
    "    # drop 'count','group' from data_y\n",
    "    data_y = data_y.drop([\"count\", \"subject\"], axis=1) \n",
    "    if str_type == \"VA_A\":\n",
    "        data_y = data_y.drop([\"valence-avg\"], axis=1) \n",
    "    elif str_type == \"VA_V\":\n",
    "        data_y = data_y.drop([\"arousal-avg\"], axis=1) \n",
    "    \n",
    "    # convert pandas to numpy \n",
    "    np_data_x = data_x.values\n",
    "    np_data_y = data_y.values\n",
    "    \n",
    "    # reshape data for tda\n",
    "    np_data_x = np.reshape(np_data_x, [len(np_data_y),dim])\n",
    "    np_data_y = np.reshape(np_data_y, [len(np_data_y),1])\n",
    "\n",
    "    #print('** np_data_x',np_data_x.shape)\n",
    "    #print('** np_data_y',np_data_y.shape)\n",
    "\n",
    "    return np_data_x, np_data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for single time analysis (Light GBM ...)\n",
    "def make_dataset_for_gbm_sub(in_data, str_type):\n",
    "    \n",
    "    data_x = in_data.drop([\"count\", \"subject\"], axis=1)\n",
    "    if \"Neutral-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"Neutral-avg\"], axis=1)\n",
    "    if \"valence-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"valence-avg\"], axis=1)\n",
    "    if \"arousal-avg\" in data_x.columns:\n",
    "        data_x = data_x.drop([\"arousal-avg\"], axis=1)\n",
    "    \n",
    "    dim = len(data_x.columns)\n",
    "    length = len(data_x)\n",
    "    \n",
    "    # convert pandas to numpy \n",
    "    np_data_x = data_x.values\n",
    "    \n",
    "    # reshape data for tda\n",
    "    np_data_x = np.reshape(np_data_x, [length, dim])\n",
    "\n",
    "    #print('** np_data_x',np_data_x.shape)\n",
    "    #print('** np_data_y',np_data_y.shape)\n",
    "\n",
    "    return np_data_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_data(data_train, data_val, models, str_type, window_time, dir_features = None):\n",
    "    log = \"split data to AU ,pose, gaze, openpose\"\n",
    "    print(log)\n",
    "    train_au, train_pose, train_lmk, train_op, train_rn = split_data(data_train)\n",
    "    val_au, val_pose, val_lmk, val_op, val_rn = split_data(data_val)\n",
    "    \n",
    "    if dir_features != None:\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_au.csv\"\n",
    "        train_au = substruct_features(train_au, file_f, str_type, window_time)\n",
    "        val_au = substruct_features(val_au, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_pose.csv\"\n",
    "        train_pose = substruct_features(train_pose, file_f, str_type, window_time)\n",
    "        val_pose = substruct_features(val_pose, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_lmk.csv\"\n",
    "        train_lmk = substruct_features(train_lmk, file_f, str_type, window_time)\n",
    "        val_lmk = substruct_features(val_lmk, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_op.csv\"\n",
    "        train_op = substruct_features(train_op, file_f, str_type, window_time)\n",
    "        val_op = substruct_features(val_op, file_f, str_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_rn.csv\"\n",
    "        train_rn = substruct_features(train_rn, file_f, str_type, window_time)\n",
    "        val_rn = substruct_features(val_rn, file_f, str_type, window_time)\n",
    "    \n",
    "    log = \"convert data(pandas) to data(numpy) for LightGBM\"\n",
    "    print(log)\n",
    "    np_train_au_x, np_train_au_y = make_dataset_for_gbm(train_au, str_type)\n",
    "    np_train_pose_x, np_train_pose_y = make_dataset_for_gbm(train_pose, str_type)\n",
    "    np_train_lmk_x, np_train_lmk_y = make_dataset_for_gbm(train_lmk, str_type)\n",
    "    np_train_op_x, np_train_op_y = make_dataset_for_gbm(train_op, str_type)\n",
    "    np_train_rn_x, np_train_rn_y = make_dataset_for_gbm(train_rn, str_type)\n",
    "    \n",
    "    np_val_au_x, np_val_au_y = make_dataset_for_gbm(val_au, str_type)\n",
    "    np_val_pose_x, np_val_pose_y = make_dataset_for_gbm(val_pose, str_type)\n",
    "    np_val_lmk_x, np_val_lmk_y = make_dataset_for_gbm(val_lmk, str_type)\n",
    "    np_val_op_x, np_val_op_y = make_dataset_for_gbm(val_op, str_type)\n",
    "    np_val_rn_x, np_val_rn_y = make_dataset_for_gbm(val_rn, str_type)\n",
    "\n",
    "    log = \"predict by au, pose, gaze, openpose, GAPP ensemple\"\n",
    "    print(log)\n",
    "    if str_type == \"EXP\":\n",
    "        pred_train_au = models[0].predict(np_train_au_x)\n",
    "        pred_train_pose = models[1].predict(np_train_pose_x)\n",
    "        pred_train_lmk = models[2].predict(np_train_lmk_x)\n",
    "        pred_train_op = models[3].predict(np_train_op_x)\n",
    "        pred_train_rn = models[4].predict(np_train_rn_x)\n",
    "        np_train_ens_x = np.column_stack((pred_train_au, pred_train_pose, pred_train_lmk,\n",
    "                                          pred_train_op, pred_train_rn))\n",
    "        pred_train_ens = models[5].predict(np_train_ens_x)\n",
    "        np_train_true = np_train_au_y.ravel()\n",
    "    else:\n",
    "        pred_train_au = models[0].predict(np_train_au_x).ravel()\n",
    "        pred_train_pose = models[1].predict(np_train_pose_x).ravel()\n",
    "        pred_train_lmk = models[2].predict(np_train_lmk_x).ravel()\n",
    "        pred_train_op = models[3].predict(np_train_op_x).ravel()\n",
    "        pred_train_rn = models[4].predict(np_train_rn_x).ravel()\n",
    "        np_train_ens_x = np.column_stack((pred_train_au, pred_train_pose, pred_train_lmk, \n",
    "                                          pred_train_op, pred_train_rn))\n",
    "        pred_train_ens = models[5].predict(np_train_ens_x).ravel()\n",
    "        np_train_true = np_train_au_y.ravel()\n",
    "        \n",
    "    if str_type == \"EXP\":\n",
    "        pred_val_au = models[0].predict(np_val_au_x)\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x)\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x)\n",
    "        pred_val_op = models[3].predict(np_val_op_x)\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x)\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk,\n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x)\n",
    "        np_val_true = np_val_au_y.ravel()\n",
    "    else:\n",
    "        pred_val_au = models[0].predict(np_val_au_x).ravel()\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x).ravel()\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x).ravel()\n",
    "        pred_val_op = models[3].predict(np_val_op_x).ravel()\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x).ravel()\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk, \n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x).ravel()\n",
    "        np_val_true = np_val_au_y.ravel()\n",
    "    \n",
    "    return np_train_true, pred_train_ens, np_val_true, pred_val_ens #pd_pred_train, pd_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict_data_sub(data_train, data_val, models, str_type, main_type, window_time, dir_features = None):\n",
    "    log = \"split data to AU ,pose, gaze, openpose\"\n",
    "    print(log)\n",
    "    train_au, train_pose, train_lmk, train_op, train_rn = split_data(data_train)\n",
    "    val_au, val_pose, val_lmk, val_op, val_rn = split_data(data_val)\n",
    "    \n",
    "    if dir_features != None:\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_au.csv\"\n",
    "        train_au = substruct_features(train_au, file_f, main_type, window_time)\n",
    "        val_au = substruct_features(val_au, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_pose.csv\"\n",
    "        train_pose = substruct_features(train_pose, file_f, main_type, window_time)\n",
    "        val_pose = substruct_features(val_pose, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_lmk.csv\"\n",
    "        train_lmk = substruct_features(train_lmk, file_f, main_type, window_time)\n",
    "        val_lmk = substruct_features(val_lmk, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_op.csv\"\n",
    "        train_op = substruct_features(train_op, file_f, main_type, window_time)\n",
    "        val_op = substruct_features(val_op, file_f, main_type, window_time)\n",
    "        file_f = dir_features + \"features_\" + str_type +\"_rn.csv\"\n",
    "        train_rn = substruct_features(train_rn, file_f, main_type, window_time)\n",
    "        val_rn = substruct_features(val_rn, file_f, main_type, window_time)\n",
    "    \n",
    "    log = \"convert data(pandas) to data(numpy) for LightGBM\"\n",
    "    print(log)\n",
    "    np_train_au_x = make_dataset_for_gbm_sub(train_au, main_type)\n",
    "    np_train_pose_x = make_dataset_for_gbm_sub(train_pose, main_type)\n",
    "    np_train_lmk_x = make_dataset_for_gbm_sub(train_lmk, main_type)\n",
    "    np_train_op_x = make_dataset_for_gbm_sub(train_op, main_type)\n",
    "    np_train_rn_x = make_dataset_for_gbm_sub(train_rn, main_type)\n",
    "    \n",
    "    np_val_au_x = make_dataset_for_gbm_sub(val_au, main_type)\n",
    "    np_val_pose_x = make_dataset_for_gbm_sub(val_pose, main_type)\n",
    "    np_val_lmk_x = make_dataset_for_gbm_sub(val_lmk, main_type)\n",
    "    np_val_op_x = make_dataset_for_gbm_sub(val_op, main_type)\n",
    "    np_val_rn_x = make_dataset_for_gbm_sub(val_rn, main_type)\n",
    "\n",
    "    log = \"predict by au, pose, gaze, openpose, GAPP ensemple\"\n",
    "    print(log)\n",
    "    if str_type == \"EXP\":\n",
    "        pred_train_au = models[0].predict(np_train_au_x)\n",
    "        pred_train_pose = models[1].predict(np_train_pose_x)\n",
    "        pred_train_lmk = models[2].predict(np_train_lmk_x)\n",
    "        pred_train_op = models[3].predict(np_train_op_x)\n",
    "        pred_train_rn = models[4].predict(np_train_rn_x)\n",
    "        np_train_ens_x = np.column_stack((pred_train_au, pred_train_pose, pred_train_lmk,\n",
    "                                          pred_train_op, pred_train_rn))\n",
    "        pred_train_ens = models[5].predict(np_train_ens_x)\n",
    "    else:\n",
    "        pred_train_au = models[0].predict(np_train_au_x).ravel()\n",
    "        pred_train_pose = models[1].predict(np_train_pose_x).ravel()\n",
    "        pred_train_lmk = models[2].predict(np_train_lmk_x).ravel()\n",
    "        pred_train_op = models[3].predict(np_train_op_x).ravel()\n",
    "        pred_train_rn = models[4].predict(np_train_rn_x).ravel()\n",
    "        np_train_ens_x = np.column_stack((pred_train_au, pred_train_pose, pred_train_lmk, \n",
    "                                          pred_train_op, pred_train_rn))\n",
    "        pred_train_ens = models[5].predict(np_train_ens_x).ravel()\n",
    "        \n",
    "    if str_type == \"EXP\":\n",
    "        pred_val_au = models[0].predict(np_val_au_x)\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x)\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x)\n",
    "        pred_val_op = models[3].predict(np_val_op_x)\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x)\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk,\n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x)\n",
    "    else:\n",
    "        pred_val_au = models[0].predict(np_val_au_x).ravel()\n",
    "        pred_val_pose = models[1].predict(np_val_pose_x).ravel()\n",
    "        pred_val_lmk = models[2].predict(np_val_lmk_x).ravel()\n",
    "        pred_val_op = models[3].predict(np_val_op_x).ravel()\n",
    "        pred_val_rn = models[4].predict(np_val_rn_x).ravel()\n",
    "        np_val_ens_x = np.column_stack((pred_val_au, pred_val_pose, pred_val_lmk, \n",
    "                                        pred_val_op, pred_val_rn))\n",
    "        pred_val_ens = models[5].predict(np_val_ens_x).ravel()\n",
    "    \n",
    "    return pred_train_ens, pred_val_ens #pd_pred_train, pd_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_CCC_for_lgbm(preds: np.ndarray, data: lgb.Dataset):\n",
    "    \"\"\"Calculate CCC\"\"\"\n",
    "    # true data\n",
    "    y_true = data.get_label()\n",
    "    # predict data\n",
    "    y_pred = preds.ravel()\n",
    "    \n",
    "    # Calc CCC\n",
    "    x_mean = y_pred.mean()\n",
    "    y_mean = y_true.mean()\n",
    "    sx2 = ((y_pred-x_mean)*(y_pred-x_mean)).mean()\n",
    "    sy2 = ((y_true-y_mean)*(y_true-y_mean)).mean()\n",
    "    sxy = ((y_pred-x_mean)*(y_true-y_mean)).mean()\n",
    "    CCC = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "    #score_acc = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    score_CCC = 2*CCC-mse\n",
    "    \n",
    "    # name, result, is_higher_better\n",
    "    return 'score_CCC', score_CCC, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_EXP_for_lgbm(preds: np.ndarray, data: lgb.Dataset):\n",
    "    \"\"\"Calculate score EXP (0.67*F1 + 0.33*acc)\"\"\"\n",
    "    # true data\n",
    "    y_true = data.get_label()\n",
    "    # reshape pred\n",
    "    N_LABELS = 7  # number of labels\n",
    "    reshaped_preds = preds.reshape(N_LABELS, len(preds) // N_LABELS)\n",
    "    # 最尤と判断したクラスを選ぶ　\n",
    "    y_pred = np.argmax(reshaped_preds, axis=0)\n",
    "    # calc\n",
    "    score_1 = f1_score(y_true, y_pred, average='macro') # weighted, macro, micro\n",
    "    score_2 = accuracy_score(y_true, y_pred)\n",
    "    score_exp = 0.67*score_1 + 0.33*score_2\n",
    "    # name, result, is_higher_better\n",
    "    return 'score_exp', score_exp, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and training\n",
    "def create_and_fit_model_GBM(train_x, train_y, validation_x, validation_y, \n",
    "                             learning_rate, num_leaves, num_iter, max_depth, bagging_fraction,\n",
    "                             feature_fraction,min_child_samples,str_type):\n",
    "    # set training, validation data\n",
    "    train_data = lgb.Dataset(train_x, label=train_y)\n",
    "    eval_data = lgb.Dataset(validation_x, label=validation_y, reference= train_data)\n",
    "    \n",
    "    # if target is expression, set parameters to learn [0~6] classification\n",
    "    if str_type == \"EXP\":\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'multiclass', #'multiclass','regression','binary'\n",
    "            #'metric':'multi_error', #binary_logloss, binary_error, multi_logloss, multi_error\n",
    "            \"metric\" : \"None\",\n",
    "            'num_leaves':num_leaves,\n",
    "            'learning_rate':learning_rate,\n",
    "            'max_depth':max_depth,\n",
    "            #'num_iterations':num_iter,\n",
    "            'verbosity': -1,\n",
    "            'num_class':7,\n",
    "            'bagging_fraction':bagging_fraction,\n",
    "            'feature_fraction':feature_fraction,\n",
    "            'min_child_samples':min_child_samples\n",
    "        }\n",
    "        # training and return model and data\n",
    "        lgb_model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=eval_data,\n",
    "            num_boost_round=400,\n",
    "            verbose_eval=0,\n",
    "            early_stopping_rounds=20,\n",
    "            feval=score_EXP_for_lgbm  # <= set custom metric function\n",
    "        )\n",
    "    # if target is VA, set parameters to learn -1 ~ 1 regression\n",
    "    else:\n",
    "        params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression', #'multiclass','regression','binary'\n",
    "            #'metric':'mse', #binary_logloss, binary_error\n",
    "            \"metric\" : \"None\",\n",
    "            'num_leaves':num_leaves,\n",
    "            'learning_rate':learning_rate,\n",
    "            'max_depth':max_depth,\n",
    "            #'num_iterations':num_iter,\n",
    "            'verbosity': -1,\n",
    "            'force_row_wise': True,\n",
    "            'bagging_fraction':bagging_fraction,\n",
    "            'feature_fraction':feature_fraction,\n",
    "            'min_child_samples':min_child_samples\n",
    "        }\n",
    "        # training and return model and data\n",
    "        lgb_model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=eval_data,\n",
    "            num_boost_round=400,\n",
    "            verbose_eval=0,\n",
    "            early_stopping_rounds=20,\n",
    "            feval=score_CCC_for_lgbm  # <= set custom metric function\n",
    "        )\n",
    "\n",
    "    return lgb_model, train_data, eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search parameter\n",
    "def search_paramter(train_x, train_y, val_x, val_y, rate_list, leaf_list, n_iter_list, depth_list, child_list, str_type):\n",
    "    model = 0\n",
    "    count = 1\n",
    "    b_frac = 1\n",
    "    f_frac = 1\n",
    "    max_count = len(rate_list)*len(leaf_list)*len(n_iter_list)*len(depth_list)*len(child_list)\n",
    "    \n",
    "    best_score = -1.000\n",
    "    best_socre_1 = -1.000\n",
    "    best_socre_2 = -1.000\n",
    "    best_rate = 0.1\n",
    "    best_leaf = 16\n",
    "    best_iter = 100\n",
    "    best_depth = 8\n",
    "    best_child = 20\n",
    "    \n",
    "    for rate in rate_list:\n",
    "        for leaf in leaf_list:\n",
    "            for n_iter in n_iter_list:\n",
    "                for depth in depth_list:\n",
    "                    for child in child_list:\n",
    "                        \n",
    "                        model, train, validation = create_and_fit_model_GBM(train_x,train_y,val_x,val_y,\n",
    "                                                                        rate,leaf,n_iter,depth,\n",
    "                                                                        b_frac,f_frac,child,\n",
    "                                                                        str_type)\n",
    "                        \n",
    "                        if str_type == \"EXP\":\n",
    "                            pred = model.predict(val_x)\n",
    "                            score_1, score_2 = eval_pred(val_y, pred, str_type)\n",
    "                            score = score_1 * 0.67 + score_2 * 0.33\n",
    "                        else:\n",
    "                            pred = model.predict(val_x).ravel()\n",
    "                            score_1, score_2 = eval_pred(val_y, pred, str_type)\n",
    "                            score = score_1\n",
    "                        \n",
    "                        log = \"{0:00004d}/{1:00004d}  Score: {2:.4f}, score_1: {3:.4f},\"\\\n",
    "                        \" score_2: {4:.4f},rate: {5:.3f}, leaf: {6}, iter: {7}, depth: {8},\"\\\n",
    "                        \" child: {9}\".format(count, max_count, score, score_1, score_2,\n",
    "                                             rate, leaf, n_iter, depth, child)\n",
    "                        print(log)\n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_socre_1 = score_1\n",
    "                            best_socre_2 = score_2\n",
    "                            best_rate = rate\n",
    "                            best_leaf = leaf\n",
    "                            best_iter = n_iter\n",
    "                            best_depth = depth\n",
    "                            best_child = child\n",
    "\n",
    "                        count = count + 1\n",
    "\n",
    "    log = \"Score: {0:.4f}, score_1: {1:.4f}, score_2: {2:.4f}, rate: {3:.3f}, leaf: {4},\"\\\n",
    "    \" iter: {5}, depth: {6}, child: {7}\".format(best_score, best_socre_1, best_socre_2,\n",
    "                                                best_rate, best_leaf, best_iter, \n",
    "                                                best_depth, best_child\n",
    "                                               )\n",
    "\n",
    "    print(log)\n",
    "    \n",
    "    return best_score, best_socre_1, best_socre_2, best_rate, best_leaf, best_iter, best_depth, best_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predict data\n",
    "def eval_pred(data_true, data_pred, str_type):\n",
    "    score_f1 = 0\n",
    "    score_acc = 0\n",
    "\n",
    "    # if target is expression, calc F1 score, accuracy\n",
    "    if str_type == \"EXP\":\n",
    "        # convert to 7-columns predict probability to 1-column predict\n",
    "        pred_tmp = np.argmax(data_pred, axis=1) # 一番大きい予測確率のクラスを予測クラスに\n",
    "        \n",
    "        ltrue = list(data_true)\n",
    "        lpred = list(pred_tmp) \n",
    "        \n",
    "        score_1 = f1_score(ltrue, lpred, average='macro') # weighted, macro, micro\n",
    "        score_2 = accuracy_score(ltrue, lpred)\n",
    "    # if target is VA, calc CCC, mse\n",
    "    else:\n",
    "        pred_tmp = data_pred\n",
    "        #pred_tmp = data_pred.round()\n",
    "        x_mean = pred_tmp.mean()\n",
    "        y_mean = data_true.mean()\n",
    "        sx2 = ((pred_tmp-x_mean)*(pred_tmp-x_mean)).mean()\n",
    "        sy2 = ((data_true-y_mean)*(data_true-y_mean)).mean()\n",
    "        sxy = ((pred_tmp-x_mean)*(data_true-y_mean)).mean()\n",
    "        score_1 = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "        #score_acc = (2 * sxy) / (sx2 + sy2 + (x_mean - y_mean) * (x_mean - y_mean))\n",
    "        score_2 = mean_squared_error(data_true, data_pred)\n",
    "    return score_1, score_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_models(dir_model, str_type, window_time):\n",
    "    ext = \"_{0}s.pickle\".format(str(window_time).zfill(2))\n",
    "    \n",
    "    model_au = load_model(dir_model + \"model_au_gbm_\" + str_type + ext)\n",
    "    model_pose = load_model(dir_model + \"model_pose_gbm_\" + str_type + ext)\n",
    "    model_lmk = load_model(dir_model + \"model_lmk_gbm_\" + str_type + ext)\n",
    "    model_op = load_model(dir_model + \"model_op_gbm_\" + str_type + ext)\n",
    "    model_rn = load_model(dir_model + \"model_rn_gbm_\" + str_type + ext)\n",
    "    model_ens = load_model(dir_model + \"model_ens_gbm_\" + str_type + ext)\n",
    "    \n",
    "    models = [model_au, model_pose, model_lmk, model_op, model_rn, model_ens]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sub_model(train_x, train_y, val_x, val_y, str_type):\n",
    "    # search parameter\n",
    "\n",
    "    # initialize\n",
    "    model = 0\n",
    "    count = 1\n",
    "    b_frac = 1\n",
    "    f_frac = 1\n",
    "    \n",
    "    # first loop\n",
    "    rate_list = [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0]\n",
    "    leaf_list = [8, 12, 16, 24, 32, 40]\n",
    "    n_iter_list = [100]\n",
    "    depth_list = [ 4, 8, 12, 16]\n",
    "    child_list = [5, 15, 30]\n",
    "\n",
    "    # search param\n",
    "    score, score_1, socre_2, rate, leaf, n_iter, depth, child = search_paramter(train_x, train_y, \n",
    "                                                            val_x, val_y, rate_list, leaf_list, \n",
    "                                                            n_iter_list, depth_list, child_list, str_type)\n",
    "\n",
    "    # next loop\n",
    "    rate_list = [rate-0.1, rate, rate+0.1]\n",
    "    leaf_list = [leaf-2, leaf, leaf+2]\n",
    "    n_iter_list = [n_iter]\n",
    "    depth_list = [ depth-2, depth, depth+2]\n",
    "    child_list = [5, 15, 30]\n",
    "\n",
    "    # search param\n",
    "    score, score_1, socre_2, rate, leaf, n_iter, depth, child = search_paramter(train_x, train_y, \n",
    "                                                            val_x, val_y, rate_list, leaf_list, \n",
    "                                                            n_iter_list, depth_list, child_list, str_type)\n",
    "\n",
    "    log = \"Score: {0:.4f}, score_1: {1:.4f}, score_2: {2:.4f}, rate: {3:.3f}, leaf: {4},\"\\\n",
    "    \" iter: {5}, depth: {6}, child: {7}\".format(score, score_1, socre_2, rate, leaf, \n",
    "                                                n_iter, depth, child )\n",
    "\n",
    "    best_rate = rate\n",
    "    best_leaf = leaf\n",
    "    best_iter = n_iter\n",
    "    best_depth = depth\n",
    "    best_child = child\n",
    "\n",
    "    print(log)\n",
    "\n",
    "    # result:\n",
    "\n",
    "    model, train, validation = create_and_fit_model_GBM(train_x,train_y, val_x, val_y,\n",
    "                                                        best_rate, best_leaf, best_iter, best_depth,\n",
    "                                                        b_frac, f_frac, best_child,\n",
    "                                                        str_type)\n",
    "\n",
    "    # calc score and print parameter\n",
    "    if str_type == \"EXP\":\n",
    "        pred = model.predict(val_x) # 7-columns\n",
    "        score_1, score_2 = eval_pred(val_y, pred, str_type)\n",
    "        score = score_1 * 0.67 + score_2 * 0.33\n",
    "    else:\n",
    "        pred = model.predict(val_x).ravel() # 1-columns to list\n",
    "        score_1, score_2 = eval_pred(val_y, pred, str_type)\n",
    "        score = score_1\n",
    "\n",
    "    log_res = \"Score: {0:.4f}, score_1: {1:.4f}, score_2: {2:.4f}, rate: {3:.3f}, leaf: {4}, iter: {5},\"\\\n",
    "    \" depth: {6}, child: {7}\".format(score, score_1, score_2, rate, leaf,n_iter, depth, child)\n",
    "    print(log)\n",
    "\n",
    "    # self predict\n",
    "    #proba_lmk_train = model.predict_proba(train_x)\n",
    "    pred_train = model.predict(train_x)\n",
    "\n",
    "    # val predict\n",
    "    #proba_lmk_val = model.predict_proba(val_x)\n",
    "    pred_val = model.predict(val_x)\n",
    "    \n",
    "    return model, pred_train, pred_val, log_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substruct_features(data, fp, str_type, window_time):\n",
    "    col = str(window_time).zfill(2) + \"s\"\n",
    "    data_f = pd.read_csv(fp)\n",
    "    \n",
    "    if str_type == \"EXP\":\n",
    "        list_f = list(data_f[col].values.ravel())\n",
    "        list_f = np.append(list_f, [\"count\", \"subject\", \"Neutral-avg\"])\n",
    "    else:\n",
    "        list_f = list(data_f[col].values.ravel())\n",
    "        list_f = np.append(list_f, [\"count\", \"subject\", \"valence-avg\", \"arousal-avg\"])\n",
    "    \n",
    "    data2 = data[list_f]\n",
    "    \n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_task_model(dir_data_train, dir_data_val, \n",
    "                               dir_model_01s, dir_model_06s, dir_model_12s, dir_model_fusion, dir_model_03s, \n",
    "                               dir_out, target, target_sub1, target_sub2, balance=False, dir_features=None):\n",
    "    str_type = target\n",
    "    str_type_sub1 = target_sub1\n",
    "    str_type_sub2 = target_sub2\n",
    "    # set output file footer\n",
    "    str_footer = \"fusion_multi_\" + str_type\n",
    "    log = \"Target type: {0}\".format(str_type)\n",
    "    print(log)\n",
    "    \n",
    "    # read models\n",
    "    model_01s = read_models(dir_model_01s, str_type, 1)\n",
    "    model_06s = read_models(dir_model_06s, str_type, 6)\n",
    "    model_12s = read_models(dir_model_12s, str_type, 12)\n",
    "    model_03s = read_models(dir_model_03s, str_type, 3)\n",
    "    \n",
    "    model_01s_sub1 = read_models(dir_model_01s, str_type_sub1, 1)\n",
    "    model_06s_sub1 = read_models(dir_model_06s, str_type_sub1, 6)\n",
    "    model_12s_sub1 = read_models(dir_model_12s, str_type_sub1, 12)\n",
    "    model_03s_sub1 = read_models(dir_model_03s, str_type_sub1, 3)\n",
    "    \n",
    "    model_01s_sub2 = read_models(dir_model_01s, str_type_sub2, 1)\n",
    "    model_06s_sub2 = read_models(dir_model_06s, str_type_sub2, 6)\n",
    "    model_12s_sub2 = read_models(dir_model_12s, str_type_sub2, 12)\n",
    "    model_03s_sub2 = read_models(dir_model_03s, str_type_sub2, 3)\n",
    "    \n",
    "    model_fusion = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type + \".pickle\")\n",
    "    model_fusion_sub1 = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type_sub1 + \".pickle\")\n",
    "    model_fusion_sub2 = load_model(dir_model_fusion + \"model_fusion_single_\" + str_type_sub2 + \".pickle\")\n",
    "    \n",
    "    # search files of training data\n",
    "    file_train = dir_data_train + \"*_01s.h5\"\n",
    "    files_train_01s = [\n",
    "        filename for filename in sorted(glob.glob(file_train))\n",
    "    ]\n",
    "    log = \"file number of train 01s: {0}\".format(len(files_train_01s))\n",
    "    print(log)\n",
    "\n",
    "    file_train = dir_data_train + \"*_06s.h5\"\n",
    "    files_train_06s = [\n",
    "        filename for filename in sorted(glob.glob(file_train))\n",
    "    ]\n",
    "    log = \"file number of train 06s: {0}\".format(len(files_train_06s))\n",
    "    print(log)\n",
    "\n",
    "    file_train = dir_data_train + \"*_12s.h5\"\n",
    "    files_train_12s = [\n",
    "        filename for filename in sorted(glob.glob(file_train))\n",
    "    ]\n",
    "    log = \"file number of train 12s: {0}\".format(len(files_train_12s))\n",
    "    print(log)\n",
    "    \n",
    "    file_train = dir_data_train + \"*_03s.h5\"\n",
    "    files_train_03s = [\n",
    "        filename for filename in sorted(glob.glob(file_train))\n",
    "    ]\n",
    "    log = \"file number of train 03s: {0}\".format(len(files_train_03s))\n",
    "    print(log)\n",
    "\n",
    "\n",
    "    # search files of validation data\n",
    "    file_val = dir_data_val + \"*_01s.h5\"\n",
    "    files_val_01s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 01s: {0}\".format(len(files_val_01s))\n",
    "    print(log)\n",
    "\n",
    "    file_val = dir_data_val + \"*_06s.h5\"\n",
    "    files_val_06s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 06s: {0}\".format(len(files_val_06s))\n",
    "    print(log)\n",
    "\n",
    "    file_val = dir_data_val + \"*_12s.h5\"\n",
    "    files_val_12s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 12s: {0}\".format(len(files_val_12s))\n",
    "    print(log)\n",
    "    \n",
    "    file_val = dir_data_val + \"*_03s.h5\"\n",
    "    files_val_03s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 03s: {0}\".format(len(files_val_03s))\n",
    "    print(log)\n",
    "\n",
    "    # create base dataset\n",
    "    log = \"data loading....\"\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_01s\"\n",
    "    #data_train = pd.read_hdf(file_train, 'key')\n",
    "    data_train_01s = crate_base_data(files_train_01s, str_type, str_time)\n",
    "    log = \"data training 01s shape: {0}\".format(data_train_01s.shape)\n",
    "    print(log)\n",
    "    data_val_01s = crate_base_data(files_val_01s, str_type, str_time)\n",
    "    log = \"data validation 01s shape: {0}\".format(data_val_01s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_06s\"\n",
    "    data_train_06s = crate_base_data(files_train_06s, str_type, str_time)\n",
    "    log = \"data training 06s shape: {0}\".format(data_train_06s.shape)\n",
    "    print(log)\n",
    "    data_val_06s = crate_base_data(files_val_06s, str_type, str_time)\n",
    "    log = \"data validation 06s shape: {0}\".format(data_val_06s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_12s\"\n",
    "    data_train_12s = crate_base_data(files_train_12s, str_type, str_time)\n",
    "    log = \"data training 12s shape: {0}\".format(data_train_12s.shape)\n",
    "    print(log)\n",
    "    data_val_12s = crate_base_data(files_val_12s, str_type, str_time)\n",
    "    log = \"data validation 12s shape: {0}\".format(data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_03s\"\n",
    "    data_train_03s = crate_base_data(files_train_03s, str_type, str_time)\n",
    "    log = \"data training 03s shape: {0}\".format(data_train_03s.shape)\n",
    "    print(log)\n",
    "    data_val_03s = crate_base_data(files_val_03s, str_type, str_time)\n",
    "    log = \"data validation 03s shape: {0}\".format(data_val_03s.shape)\n",
    "    print(log)\n",
    "\n",
    "    \n",
    "    # adjust data shape (same frame)\n",
    "    log = \"adjust data shape (same frame)\"\n",
    "    print(log)\n",
    "    log = \"train data shape) 01s: {0}, 06s: {1}, 12s: {2}, 06s: {3}\".format(data_train_01s.shape, data_train_06s.shape,\n",
    "                                                                  data_train_12s.shape, data_train_03s.shape)\n",
    "    print(log)\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}, 06s: {3}\".format(data_val_01s.shape, data_val_06s.shape, \n",
    "                                                                data_val_12s.shape, data_val_03s.shape)\n",
    "    print(log)\n",
    "\n",
    "    length_columns = len(data_train_01s.columns)\n",
    "    base_columns = data_train_01s.columns\n",
    "\n",
    "    data_train_01s.columns = data_train_01s.columns + \"_01s\"\n",
    "    data_train_06s.columns = data_train_06s.columns + \"_06s\"\n",
    "    data_train_12s.columns = data_train_12s.columns + \"_12s\"\n",
    "    data_train_03s.columns = data_train_03s.columns + \"_03s\"\n",
    "\n",
    "    data_val_01s.columns = data_val_01s.columns + \"_01s\"\n",
    "    data_val_06s.columns = data_val_06s.columns + \"_06s\"\n",
    "    data_val_12s.columns = data_val_12s.columns + \"_12s\"\n",
    "    data_val_03s.columns = data_val_03s.columns + \"_03s\"\n",
    "\n",
    "    data_train = pd.concat([data_train_01s, data_train_06s, data_train_12s, data_train_03s], axis=1)\n",
    "    if str_type == \"EXP\":\n",
    "        data_train = data_train.loc[data_train[\"Neutral-avg_01s\"]>=0]\n",
    "        data_train = data_train.loc[data_train[\"Neutral-avg_06s\"]>=0]\n",
    "        data_train = data_train.loc[data_train[\"Neutral-avg_12s\"]>=0]\n",
    "        data_train = data_train.loc[data_train[\"Neutral-avg_03s\"]>=0]\n",
    "    data_train = data_train.dropna(how='any')\n",
    "    train_index = data_train.index\n",
    "\n",
    "    data_val = pd.concat([data_val_01s, data_val_06s, data_val_12s, data_val_03s], axis=1)\n",
    "    if str_type == \"EXP\":\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_01s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_06s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_12s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_03s\"]>=0]\n",
    "    data_val = data_val.dropna(how='any')\n",
    "    val_index = data_val.index\n",
    "    \n",
    "    if balance == True:\n",
    "        data_train = data_balance_multi_term(data_train, str_type)\n",
    "\n",
    "    data_train_01s = data_train.iloc[:,0:length_columns]\n",
    "    data_train_06s = data_train.iloc[:,length_columns:length_columns*2]\n",
    "    data_train_12s = data_train.iloc[:,length_columns*2:length_columns*3]\n",
    "    data_train_03s = data_train.iloc[:,length_columns*3:length_columns*4]\n",
    "\n",
    "    data_val_01s = data_val.iloc[:,0:length_columns]\n",
    "    data_val_06s = data_val.iloc[:,length_columns:length_columns*2]\n",
    "    data_val_12s = data_val.iloc[:,length_columns*2:length_columns*3]\n",
    "    data_val_03s = data_val.iloc[:,length_columns*3:length_columns*4]\n",
    "\n",
    "    data_train_01s.columns = base_columns\n",
    "    data_train_06s.columns = base_columns\n",
    "    data_train_12s.columns = base_columns\n",
    "    data_train_03s.columns = base_columns\n",
    "\n",
    "    data_val_01s.columns = base_columns\n",
    "    data_val_06s.columns = base_columns\n",
    "    data_val_12s.columns = base_columns\n",
    "    data_val_03s.columns = base_columns\n",
    "\n",
    "    log = \"train data shape) 01s: {0}, 06s: {1}, 12s: {2}, 03s: {3}\".format(data_train_01s.shape, data_train_06s.shape, \n",
    "                                                                            data_train_12s.shape, data_train_03s.shape)\n",
    "    print(log)\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}, 03s: {3}\".format(data_val_01s.shape, data_val_06s.shape,\n",
    "                                                                          data_val_12s.shape, data_val_03s.shape)\n",
    "    print(log)\n",
    "    \n",
    "    # main\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    #pred_train_01s, pred_val_01s = predict_data(data_train_01s, data_val_01s, model_01s, str_type)\n",
    "    np_train_true_01s, pred_train_ens_01s, np_val_true_01s, pred_val_ens_01s = predict_data(data_train_01s, data_val_01s,\n",
    "                                                                                            model_01s, str_type,\n",
    "                                                                                            window_time, dir_features)\n",
    "    log = \"01s pred shape) train: {0}, val: {1}\".format(pred_train_ens_01s.shape, pred_val_ens_01s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    #pred_train_06s, pred_val_06s = predict_data(data_train_06s, data_val_06s, model_06s, str_type)\n",
    "    np_train_true_06s, pred_train_ens_06s, np_val_true_06s, pred_val_ens_06s = predict_data(data_train_06s, data_val_06s,\n",
    "                                                                                            model_06s, str_type,\n",
    "                                                                                            window_time, dir_features)\n",
    "    log = \"06s pred shape) train: {0}, val: {1}\".format(pred_train_ens_06s.shape, pred_val_ens_06s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    #pred_train_12s, pred_val_12s = predict_data(data_train_12s, data_val_12s, model_12s, str_type)\n",
    "    np_train_true_12s, pred_train_ens_12s, np_val_true_12s, pred_val_ens_12s = predict_data(data_train_12s, data_val_12s,\n",
    "                                                                                            model_12s, str_type,\n",
    "                                                                                            window_time, dir_features)\n",
    "    log = \"12s pred shape) train: {0}, val: {1}\".format(pred_train_ens_12s.shape, pred_val_ens_12s.shape)\n",
    "    print(log)\n",
    "    \n",
    "    # 03s\n",
    "    window_time = 3\n",
    "    #pred_train_12s, pred_val_12s = predict_data(data_train_12s, data_val_12s, model_12s, str_type)\n",
    "    np_train_true_03s, pred_train_ens_03s, np_val_true_03s, pred_val_ens_03s = predict_data(data_train_03s, data_val_03s,\n",
    "                                                                                            model_03s, str_type,\n",
    "                                                                                            window_time, dir_features)\n",
    "    log = \"03s pred shape) train: {0}, val: {1}\".format(pred_train_ens_03s.shape, pred_val_ens_03s.shape)\n",
    "    print(log)\n",
    "    \n",
    "    # sub1 (with fusion single)\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    #pred_train_01s, pred_val_01s = predict_data(data_train_01s, data_val_01s, model_01s, str_type)\n",
    "    pred_train_ens_01s_sub1, pred_val_ens_01s_sub1 = predict_data_sub(data_train_01s, data_val_01s,\n",
    "                                                                      model_01s_sub1, str_type_sub1, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"01s pred shape) train: {0}, val: {1}\".format(pred_train_ens_01s_sub1.shape, pred_val_ens_01s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    pred_train_ens_06s_sub1, pred_val_ens_06s_sub1 = predict_data_sub(data_train_06s, data_val_06s,\n",
    "                                                                      model_06s_sub1, str_type_sub1, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"06s pred shape) train: {0}, val: {1}\".format(pred_train_ens_06s_sub1.shape, pred_val_ens_06s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    pred_train_ens_12s_sub1, pred_val_ens_12s_sub1 = predict_data_sub(data_train_12s, data_val_12s,\n",
    "                                                                      model_12s_sub1, str_type_sub1, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"12s pred shape) train: {0}, val: {1}\".format(pred_train_ens_12s_sub1.shape, pred_val_ens_12s_sub1.shape)\n",
    "    print(log)\n",
    "    \n",
    "    # 03s\n",
    "    window_time = 3\n",
    "    pred_train_ens_03s_sub1, pred_val_ens_03s_sub1 = predict_data_sub(data_train_03s, data_val_03s,\n",
    "                                                                      model_03s_sub1, str_type_sub1, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"03s pred shape) train: {0}, val: {1}\".format(pred_train_ens_03s_sub1.shape, pred_val_ens_03s_sub1.shape)\n",
    "    print(log)\n",
    "\n",
    "\n",
    "    # fusion sub1\n",
    "    # stacked predict data: train\n",
    "    stack_pred = np.column_stack((pred_train_ens_01s_sub1, pred_train_ens_06s_sub1, pred_train_ens_12s_sub1, pred_train_ens_03s_sub1))\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((pred_val_ens_01s_sub1, pred_val_ens_06s_sub1, pred_val_ens_12s_sub1, pred_val_ens_03s_sub1))\n",
    "\n",
    "    if str_type_sub1 == \"EXP\":\n",
    "        pred_train_sub1 = model_fusion_sub1.predict(stack_pred)\n",
    "        pred_val_sub1 = model_fusion_sub1.predict(stack_pred_val)\n",
    "    else:\n",
    "        pred_train_sub1 = model_fusion_sub1.predict(stack_pred).ravel()\n",
    "        pred_val_sub1 = model_fusion_sub1.predict(stack_pred_val).ravel()\n",
    "\n",
    "    # sub2 (with fusion single)\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    #pred_train_01s, pred_val_01s = predict_data(data_train_01s, data_val_01s, model_01s, str_type)\n",
    "    pred_train_ens_01s_sub2, pred_val_ens_01s_sub2 = predict_data_sub(data_train_01s, data_val_01s,\n",
    "                                                                      model_01s_sub2, str_type_sub2, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"01s pred shape) train: {0}, val: {1}\".format(pred_train_ens_01s_sub2.shape, pred_val_ens_01s_sub2.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    pred_train_ens_06s_sub2, pred_val_ens_06s_sub2 = predict_data_sub(data_train_06s, data_val_06s,\n",
    "                                                                      model_06s_sub2, str_type_sub2, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"06s pred shape) train: {0}, val: {1}\".format(pred_train_ens_06s_sub2.shape, pred_val_ens_06s_sub2.shape)\n",
    "    print(log)\n",
    "\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    pred_train_ens_12s_sub2, pred_val_ens_12s_sub2 = predict_data_sub(data_train_12s, data_val_12s,\n",
    "                                                                      model_12s_sub2, str_type_sub2, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"12s pred shape) train: {0}, val: {1}\".format(pred_train_ens_12s_sub2.shape, pred_val_ens_12s_sub2.shape)\n",
    "    print(log)\n",
    "    \n",
    "    # 03s\n",
    "    window_time = 3\n",
    "    pred_train_ens_03s_sub2, pred_val_ens_03s_sub2 = predict_data_sub(data_train_03s, data_val_03s,\n",
    "                                                                      model_03s_sub2, str_type_sub2, str_type,\n",
    "                                                                      window_time, dir_features)\n",
    "    log = \"03s pred shape) train: {0}, val: {1}\".format(pred_train_ens_03s_sub2.shape, pred_val_ens_03s_sub2.shape)\n",
    "    print(log)\n",
    "    \n",
    "\n",
    "    # fusion sub2\n",
    "    # stacked predict data: train\n",
    "    stack_pred = np.column_stack((pred_train_ens_01s_sub2, pred_train_ens_06s_sub2, pred_train_ens_12s_sub2, pred_train_ens_03s_sub2))\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((pred_val_ens_01s_sub2, pred_val_ens_06s_sub2, pred_val_ens_12s_sub2, pred_val_ens_03s_sub2))\n",
    "\n",
    "    if str_type_sub2 == \"EXP\":\n",
    "        pred_train_sub2 = model_fusion_sub2.predict(stack_pred)\n",
    "        pred_val_sub2 = model_fusion_sub2.predict(stack_pred_val)\n",
    "    else:\n",
    "        pred_train_sub2 = model_fusion_sub2.predict(stack_pred).ravel()\n",
    "        pred_val_sub2 = model_fusion_sub2.predict(stack_pred_val).ravel()\n",
    "\n",
    "    # ensemble    \n",
    "    # stacked predict data: train\n",
    "    stack_pred = np.column_stack((pred_train_ens_01s, pred_train_ens_06s, pred_train_ens_12s, pred_train_ens_03s,\n",
    "                                  pred_train_sub1, pred_train_sub2))\n",
    "\n",
    "    # stacked predict data: validation\n",
    "    stack_pred_val = np.column_stack((pred_val_ens_01s, pred_val_ens_06s, pred_val_ens_12s, pred_val_ens_03s,\n",
    "                                      pred_val_sub1, pred_val_sub2))\n",
    "    \n",
    "    # generate single-term ensemble model\n",
    "    model_ens, pred_train_ens, pred_val_ens, log_res_ens = generate_sub_model(stack_pred, np_train_true_01s, \n",
    "                                                                              stack_pred_val, np_val_true_01s,\n",
    "                                                                              str_type)\n",
    "\n",
    "    # save model\n",
    "    f = dir_out + 'model_' + str_footer + '.pickle'\n",
    "    with open(f, mode='wb') as fp:\n",
    "        pickle.dump(model_ens, fp)\n",
    "    \n",
    "    #save log\n",
    "    print(log_res_ens)\n",
    "\n",
    "    file_result = dir_out + 'result_' + str_footer + '.txt'\n",
    "    with open(file_result, mode='w') as f:\n",
    "        f.write(log_res_ens)\n",
    "\n",
    "        \n",
    "    log = \"*** FINISHED *** Target type: {0}\".format(str_type)\n",
    "    print(log)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
