{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluate multi time-window model taht is fusion model of multi target models\n",
    " * select type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * select sub1, sub2 type: \"EXP\", \"VA_V\", \"VA_A\"\n",
    " * evaluate validation per frame\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sklearn #機械学習のライブラリ\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score,mean_squared_error,f1_score\n",
    "from statistics import mean, median,variance,stdev\n",
    "import math\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base dataset: concat au csv & add label, file count & drop unnecessary columns\n",
    "#   str_substract_time: ex. '02s' , str_search_key: ex. '(Subject_).*.csv'\n",
    "#   cut_start: trim data (X sec from start), cut_end: trim data (X sec from end)\n",
    "def crate_base_data(data_file_names, str_type, str_time):\n",
    "    # create empty dataframe (base dataframe)\n",
    "    #data = pd.DataFrame()\n",
    "    count = 0\n",
    "    max_count = len(data_file_names)\n",
    "    \n",
    "    data_list = [pd.DataFrame()] # <- dummy\n",
    "    \n",
    "    for  data_file in data_file_names:\n",
    "        # read au csv\n",
    "        if os.path.isfile(data_file) and os.path.getsize(data_file) > 32:\n",
    "            #print(os.path.getsize(data_file))\n",
    "            #data_tmp = pd.read_csv(data_file)\n",
    "            data_tmp = pd.read_hdf(data_file)\n",
    "        else:\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        if (len(data_tmp)<1):\n",
    "            count = count+1\n",
    "            continue\n",
    "        \n",
    "        # create column - 'count', 'Label', 'subject' (default: 0)\n",
    "        data_tmp[\"count\"] = 0\n",
    "        data_tmp[\"subject\"] = \"sample\"\n",
    "\n",
    "        # convert filename to 'subject'\n",
    "        name_train = os.path.splitext(os.path.basename(data_file))[0].replace(str_time,'')\n",
    "        #print(name_train)\n",
    "\n",
    "        #print(data_temp)\n",
    "        # get and set Label value\n",
    "        data_tmp[\"count\"]  = count\n",
    "        data_tmp[\"subject\"] = name_train\n",
    "        \n",
    "        # drop unnecessary columns\n",
    "        # ' frame-avg',' face_id-avg,' timestamp-avg',' confidence-avg,' success-avg','frame-std',' face_id-std',' confidence-std',' success-std'\n",
    "        data_tmp = data_tmp.drop(['frame-avg',' face_id-avg',' timestamp-avg',' confidence-avg',' success-avg',\n",
    "                                  'frame-std',' face_id-std',' timestamp-std',' confidence-std',' success-std',\n",
    "                                  'frame-range', ' face_id-range', ' timestamp-range', ' confidence-range', ' success-range',\n",
    "                                  'frame-slope', ' face_id-slope', ' timestamp-slope', ' confidence-slope', ' success-slope',\n",
    "                                  'Unnamed: 0-avg', 'Unnamed: 0-std', 'Unnamed: 0-range', 'Unnamed: 0-slope'\n",
    "                               ], axis=1)\n",
    "        if str_type == \"EXP\":\n",
    "            data_tmp = data_tmp.drop(['Neutral-std','Neutral-range','Neutral-slope'], axis=1)\n",
    "        else:\n",
    "            data_tmp = data_tmp.drop(['arousal-std', 'arousal-range', 'arousal-slope', \n",
    "                                     'valence-std', 'valence-range', 'valence-slope'], axis=1)\n",
    "\n",
    "        # append created data to base dataframe\n",
    "        #data = data.append(data_tmp)\n",
    "        data_list.append(data_tmp)\n",
    "\n",
    "        log = 'count: {0}, name: {1}, data shape: {2}'.format(count, name_train, data_tmp.shape)\n",
    "        print(log)\n",
    "        count = count + 1\n",
    "    # finish\n",
    "    del data_list[0]\n",
    "    data = pd.concat([x for x in data_list])\n",
    "    \n",
    "    log = '**** finished creating base dataset, data shape: {0}'.format(data.shape)\n",
    "    print(log)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_model):\n",
    "    with open(file_model, mode='rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split base data to <au>, <gaze and pose>, <eye_landmark, 2d landmark, 3d landmark>\n",
    "# ** 'count','label','subject' is contained in all splits\n",
    "def split_data_no_label(in_data):\n",
    "    # au data\n",
    "    df_au = in_data.loc[:, in_data.columns.str.contains(\"AU\") ]\n",
    "    #df_au = df_au.join(df_lable)\n",
    "    print(\"AU data shape: \",df_au.shape)\n",
    "\n",
    "    # gaze and pose data **** temp pose\n",
    "    df_pose = in_data.loc[:, in_data.columns.str.contains(\"pose_\") ]\n",
    "    #df_pose = df_pose.join(df_lable)\n",
    "    print(\"Gaze & Pose data shape: \",df_pose.shape)\n",
    "    \n",
    "    # eye_landmark, 2d landmark, 3d landmark data **** temp gaze\n",
    "    df_lmk = in_data.loc[:, in_data.columns.str.contains(\"gaze\")]\n",
    "    #df_lmk = df_lmk.join(df_lable)\n",
    "    print(\"Landmark data shape: \",df_lmk.shape)\n",
    "    \n",
    "    # openpose\n",
    "    #df_op = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze\")]\n",
    "    df_op = in_data.loc[:, in_data.columns.str.contains(\"hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c\")]\n",
    "    print(\"Opepose data shape: \",df_op.shape)\n",
    "    \n",
    "    # resnet\n",
    "    df_rn = in_data.loc[:, ~in_data.columns.str.contains(\"AU|pose_|gaze|hand_flag|0x|0y|0c|1x|1y|1c|2x|2y|2c|3x|3y|3c|4x|4y|4c|5x|5y|5c|6x|6y|6c|7x|7y|7c|8x|8y|8c|9x|9y|9c|10x|10y|10c|11x|11y|11c|12x|12y|12c|13x|13y|13c|14x|14y|14c|15x|15y|15c|16x|16y|16c|17x|17y|17c|18x|18y|18c|19x|19y|19c|20x|20y|20c|21x|21y|21c|22x|22y|22c|23x|23y|23c|24x|24y|24c|count|subject|Neutral|valence|arousal\")]\n",
    "    print(\"Resnet data shape: \",df_rn.shape)\n",
    "    \n",
    "    print(\"** end **\")\n",
    "    return df_au,df_pose,df_lmk,df_op, df_rn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def calc_imprtances(data_val, models, str_type, window_time):\n",
    "    log = \"split data to AU ,pose, gaze, openpose\"\n",
    "    print(log)\n",
    "    val_au, val_pose, val_lmk, val_op, val_rn = split_data_no_label(data_val)\n",
    "\n",
    "    col_au = list(val_au.columns)\n",
    "    col_pose = list(val_pose.columns)\n",
    "    col_lmk = list(val_lmk.columns)\n",
    "    col_op = list(val_op.columns)\n",
    "    col_rn = list(val_rn.columns)\n",
    "\n",
    "    str_col = str(window_time).zfill(2) + \"s\"\n",
    "    importance_au = pd.DataFrame(models[0].feature_importance(), index=col_au, columns=[str_col])\n",
    "    importance_pose = pd.DataFrame(models[1].feature_importance(), index=col_pose, columns=[str_col])\n",
    "    importance_lmk = pd.DataFrame(models[2].feature_importance(), index=col_lmk, columns=[str_col])\n",
    "    importance_op = pd.DataFrame(models[3].feature_importance(), index=col_op, columns=[str_col])\n",
    "    importance_rn = pd.DataFrame(models[4].feature_importance(), index=col_rn, columns=[str_col])\n",
    "    \n",
    "    return importance_au, importance_pose, importance_lmk, importance_op, importance_rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_models(dir_model, str_type, window_time):\n",
    "    ext = \"_{0}s.pickle\".format(str(window_time).zfill(2))\n",
    "    \n",
    "    model_au = load_model(dir_model + \"model_au_gbm_\" + str_type + ext)\n",
    "    model_pose = load_model(dir_model + \"model_pose_gbm_\" + str_type + ext)\n",
    "    model_lmk = load_model(dir_model + \"model_lmk_gbm_\" + str_type + ext)\n",
    "    model_op = load_model(dir_model + \"model_op_gbm_\" + str_type + ext)\n",
    "    model_rn = load_model(dir_model + \"model_rn_gbm_\" + str_type + ext)\n",
    "    model_ens = load_model(dir_model + \"model_ens_gbm_\" + str_type + ext)\n",
    "    \n",
    "    models = [model_au, model_pose, model_lmk, model_op, model_rn, model_ens]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_get_importances(dir_validation, dir_model_01s, dir_model_06s, dir_model_12s, dir_out):\n",
    "    str_type = \"VA_A\"\n",
    "    str_type_sub1 = \"VA_V\"\n",
    "    str_type_sub2 = \"EXP\"\n",
    "    \n",
    "    # read models\n",
    "    model_01s = read_models(dir_model_01s, str_type, 1)\n",
    "    model_06s = read_models(dir_model_06s, str_type, 6)\n",
    "    model_12s = read_models(dir_model_12s, str_type, 12)\n",
    "    \n",
    "    model_01s_sub1 = read_models(dir_model_01s, str_type_sub1, 1)\n",
    "    model_06s_sub1 = read_models(dir_model_06s, str_type_sub1, 6)\n",
    "    model_12s_sub1 = read_models(dir_model_12s, str_type_sub1, 12)\n",
    "    \n",
    "    model_01s_sub2 = read_models(dir_model_01s, str_type_sub2, 1)\n",
    "    model_06s_sub2 = read_models(dir_model_06s, str_type_sub2, 6)\n",
    "    model_12s_sub2 = read_models(dir_model_12s, str_type_sub2, 12)\n",
    "    \n",
    "    # search files of validation data\n",
    "    file_val = dir_validation + \"*_01s.h5\"\n",
    "    files_val_01s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 01s: {0}\".format(len(files_val_01s))\n",
    "    print(log)\n",
    "    files_val_01s = files_val_01s[0:2]\n",
    "\n",
    "    file_val = dir_validation + \"*_06s.h5\"\n",
    "    files_val_06s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 06s: {0}\".format(len(files_val_06s))\n",
    "    print(log)\n",
    "    files_val_06s = files_val_06s[0:2]\n",
    "\n",
    "    file_val = dir_validation + \"*_12s.h5\"\n",
    "    files_val_12s = [\n",
    "        filename for filename in sorted(glob.glob(file_val))\n",
    "    ]\n",
    "    log = \"file number of val 12s: {0}\".format(len(files_val_12s))\n",
    "    print(log)\n",
    "    files_val_12s = files_val_12s[0:2]\n",
    "    \n",
    "    # create base dataset\n",
    "    log = \"data loading....\"\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_01s\"\n",
    "    #data_train = pd.read_hdf(file_train, 'key')\n",
    "    data_val_01s = crate_base_data(files_val_01s, str_type, str_time)\n",
    "    log = \"data validation 01s shape: {0}\".format(data_val_01s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_06s\"\n",
    "    data_val_06s = crate_base_data(files_val_06s, str_type, str_time)\n",
    "    log = \"data validation 06s shape: {0}\".format(data_val_06s.shape)\n",
    "    print(log)\n",
    "\n",
    "    str_time = \"_12s\"\n",
    "    data_val_12s = crate_base_data(files_val_12s, str_type, str_time)\n",
    "    log = \"data validation 12s shape: {0}\".format(data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    #data_val = pd.read_hdf(file_val, 'key')\n",
    "\n",
    "    # create base dataset\n",
    "    log = \"finished data loading\"\n",
    "    print(log)\n",
    "    \n",
    "    # adjust data shape (same frame)\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}\".format(data_val_01s.shape, data_val_06s.shape, data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    length_columns = len(data_val_01s.columns)\n",
    "    base_columns = data_val_01s.columns\n",
    "\n",
    "    data_val_01s.columns = data_val_01s.columns + \"_01s\"\n",
    "    data_val_06s.columns = data_val_06s.columns + \"_06s\"\n",
    "    data_val_12s.columns = data_val_12s.columns + \"_12s\"\n",
    "\n",
    "    data_val = pd.concat([data_val_01s, data_val_06s, data_val_12s], axis=1)\n",
    "    if str_type == \"EXP\":\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_01s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_06s\"]>=0]\n",
    "        data_val = data_val.loc[data_val[\"Neutral-avg_12s\"]>=0]\n",
    "    data_val = data_val.dropna(how='any')\n",
    "    val_index = data_val.index\n",
    "\n",
    "    data_val_01s = data_val.iloc[:,0:length_columns]\n",
    "    data_val_06s = data_val.iloc[:,length_columns:length_columns*2]\n",
    "    data_val_12s = data_val.iloc[:,length_columns*2:length_columns*3]\n",
    "\n",
    "    data_val_01s.columns = base_columns\n",
    "    data_val_06s.columns = base_columns\n",
    "    data_val_12s.columns = base_columns\n",
    "\n",
    "    log = \"val data shape) 01s: {0}, 06s: {1}, 12s: {2}\".format(data_val_01s.shape, data_val_06s.shape, data_val_12s.shape)\n",
    "    print(log)\n",
    "\n",
    "    # VA_A:\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    au_1, pose_1, lmk_1, op_1, rn_1 = calc_imprtances(data_val_01s, model_01s, str_type, window_time)\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    au_6, pose_6, lmk_6, op_6, rn_6 = calc_imprtances(data_val_06s, model_06s, str_type, window_time)\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    au_12, pose_12, lmk_12, op_12, rn_12 = calc_imprtances(data_val_12s, model_12s, str_type, window_time)\n",
    "    \n",
    "    fp = dir_out + \"importances_\" + str_type + \"_au.csv\"\n",
    "    au = pd.concat([au_1, au_6, au_12], axis=1)\n",
    "    au.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type + \"_pose.csv\"\n",
    "    pose = pd.concat([pose_1, pose_6, pose_12], axis=1)\n",
    "    pose.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type + \"_lmk.csv\"\n",
    "    lmk = pd.concat([lmk_1, lmk_6, lmk_12], axis=1)\n",
    "    lmk.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type + \"_op.csv\"\n",
    "    op = pd.concat([op_1, op_6, op_12], axis=1)\n",
    "    op.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type + \"_en.csv\"\n",
    "    rn = pd.concat([rn_1, rn_6, rn_12], axis=1)\n",
    "    rn.to_csv(fp)\n",
    "    \n",
    "    # VA_V:\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    au_1, pose_1, lmk_1, op_1, rn_1 = calc_imprtances(data_val_01s, model_01s_sub1, str_type_sub1, window_time)\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    au_6, pose_6, lmk_6, op_6, rn_6 = calc_imprtances(data_val_06s, model_06s_sub1, str_type_sub1, window_time)\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    au_12, pose_12, lmk_12, op_12, rn_12 = calc_imprtances(data_val_12s, model_12s_sub1, str_type_sub1, window_time)\n",
    "    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub1 + \"_au.csv\"\n",
    "    au = pd.concat([au_1, au_6, au_12], axis=1)\n",
    "    au.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub1 + \"_pose.csv\"\n",
    "    pose = pd.concat([pose_1, pose_6, pose_12], axis=1)\n",
    "    pose.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub1 + \"_lmk.csv\"\n",
    "    lmk = pd.concat([lmk_1, lmk_6, lmk_12], axis=1)\n",
    "    lmk.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub1 + \"_op.csv\"\n",
    "    op = pd.concat([op_1, op_6, op_12], axis=1)\n",
    "    op.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub1 + \"_rn.csv\"\n",
    "    rn = pd.concat([rn_1, rn_6, rn_12], axis=1)\n",
    "    rn.to_csv(fp)    \n",
    "\n",
    "    # EXP:\n",
    "    # 01s\n",
    "    window_time = 1\n",
    "    au_1, pose_1, lmk_1, op_1, rn_1 = calc_imprtances(data_val_01s, model_01s_sub2, str_type_sub2, window_time)\n",
    "    # 06s\n",
    "    window_time = 6\n",
    "    au_6, pose_6, lmk_6, op_6, rn_6 = calc_imprtances(data_val_06s, model_06s_sub2, str_type_sub2, window_time)\n",
    "    # 12s\n",
    "    window_time = 12\n",
    "    au_12, pose_12, lmk_12, op_12, rn_12 = calc_imprtances(data_val_12s, model_12s_sub2, str_type_sub2, window_time)\n",
    "    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub2 + \"_au.csv\"\n",
    "    au = pd.concat([au_1, au_6, au_12], axis=1)\n",
    "    au.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub2 + \"_pose.csv\"\n",
    "    pose = pd.concat([pose_1, pose_6, pose_12], axis=1)\n",
    "    pose.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub2 + \"_lmk.csv\"\n",
    "    lmk = pd.concat([lmk_1, lmk_6, lmk_12], axis=1)\n",
    "    lmk.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub2 + \"_op.csv\"\n",
    "    op = pd.concat([op_1, op_6, op_12], axis=1)\n",
    "    op.to_csv(fp)    \n",
    "    fp = dir_out + \"importances_\" + str_type_sub2 + \"_rn.csv\"\n",
    "    rn = pd.concat([rn_1, rn_6, rn_12], axis=1)\n",
    "    rn.to_csv(fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root folder\n",
    "dir_submit = str(Path().resolve())\n",
    "dir_base = str(Path(Path().resolve()).parent) + \"\\\\base_data\"\n",
    "\n",
    "# merged data folder (va, exp : train, validation)\n",
    "dir_data_va_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_VA_roll\\\\Validation_Frame\\\\\"\n",
    "dir_data_exp_val = dir_base + \"\\\\Merged_with_resnet\\\\Merged_EXP_roll\\\\Validation_Frame\\\\\"\n",
    "\n",
    "# set model folder: 01, 06, 12s window\n",
    "dir_model_01s = dir_submit + \"\\\\models\\\\t01\\\\\"\n",
    "dir_model_06s = dir_submit + \"\\\\models\\\\t06\\\\\"\n",
    "dir_model_12s = dir_submit + \"\\\\models\\\\t12\\\\\"\n",
    "\n",
    "# set output folder\n",
    "dir_out = dir_submit + \"\\\\models\\\\ensemble\\\\\"\n",
    "if os.path.isdir(dir_out) == False:\n",
    "    os.makedirs(dir_out)\n",
    "\n",
    "dir_data_val = dir_data_va_val\n",
    "run_get_importances(dir_data_val, dir_model_01s, dir_model_06s, dir_model_12s, dir_out)\n",
    "\n",
    "print(\"*** finished ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
